{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# AI Co-Scientist for Pharma R&D with Open-Source LLMs\n",
    "\n",
    "This notebook implements a multi-agent AI system designed to function as a \"co-scientist,\" accelerating experimental design in pharmaceutical R&D. The system automates hypothesis generation, grounds proposals in data using tools, and uses a tiered approach of different AI agents to refine and validate experimental protocols.\n",
    "\n",
    "This implementation is a high-fidelity recreation of the architecture from OpenAI's Model Selection Guide, fully adapted to work with customizable open-source LLMs and enhanced with advanced evaluation metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-discussion",
   "metadata": {},
   "source": [
    "## Architecture Overview and Enhancements\n",
    "\n",
    "We adopt the original's robust multi-agent workflow, which uses different models for specialized tasks in a pattern of \"escalation of intelligence.\"\n",
    "\n",
    "**Core Workflow:**\n",
    "1.  **Scientist Input:** A human scientist defines the research goal, compound, and constraints.\n",
    "2.  **Ideation:** Multiple parallel agents, each with a specific role, generate diverse experimental plans using a fast, cost-effective LLM.\n",
    "3.  **Tournament Ranking:** The generated plans are compared pairwise to efficiently select the most promising candidate.\n",
    "4.  **Deep Critique:** The winning plan is escalated to a more powerful LLM for rigorous scientific review and refinement.\n",
    "5.  **Safety Check:** A final check is performed to identify potential hazards.\n",
    "6.  **Human Review:** The final, refined protocol is presented to the human scientist for ultimate approval.\n",
    "7.  **Execution & Learning:** Experimental results are analyzed to create structured knowledge for future runs.\n",
    "\n",
    "**Our Enhancements:**\n",
    "- **An Advanced Quality Scoring Agent:** A new evaluation agent scores the final protocol on five dimensions: `Scientific Validity`, `Feasibility`, `Innovation`, `Cost-Effectiveness`, and `Clarity & Reproducibility`.\n",
    "- **A Comprehensive Final Summary:** We aggregate all operational metrics and our new quality scores into a single DataFrame, offering a holistic view of the entire process for each run."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-config",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU openai pandas tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-and-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass, field\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "API_KEY = \"API_KEY\"\n",
    "BASE_URL = \"https://api.studio.nebius.com/v1/\"\n",
    "\n",
    "MODEL_IDEATE = \"Qwen/Qwen3-4B-fast\"\n",
    "MODEL_CRITIQUE = \"Qwen/Qwen3-235B-A22B\"\n",
    "MODEL_SAFETY = \"Qwen/Qwen3-14B\"\n",
    "MODEL_EVALUATE = \"Qwen/Qwen3-235B-A22B\"\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "metrics_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils-md",
   "metadata": {},
   "source": [
    "## 2. Utilities and Core Logic\n",
    "\n",
    "This section replicates the essential helper functions from the original `agent_utils.py` file, including the `Context` dataclass for managing run parameters and the `log_json` function for structured logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "utils-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Context:\n",
    "    compound: str\n",
    "    goal: str\n",
    "    budget: float\n",
    "    time_h: int\n",
    "    previous: str\n",
    "    client: OpenAI\n",
    "    run_id: str = field(default_factory=lambda: uuid.uuid4().hex[:8])\n",
    "    # We add this field to store the critique result for the final summary\n",
    "    critique_recommendation: Optional[str] = None\n",
    "\n",
    "    def prompt_vars(self):\n",
    "        return {\n",
    "            \"compound\": self.compound,\n",
    "            \"goal\": self.goal,\n",
    "            \"budget\": self.budget,\n",
    "            \"time_h\": self.time_h,\n",
    "            \"previous\": self.previous,\n",
    "        }\n",
    "\n",
    "def log_json(stage: str, data: Any, ctx: Context):\n",
    "    Path(\"logs\").mkdir(exist_ok=True)\n",
    "    p = Path(\"logs\") / f\"{ctx.run_id}.log\"\n",
    "    with p.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        log_entry = {\"ts\": time.time(), \"stage\": stage, \"data\": data}\n",
    "        f.write(json.dumps(log_entry, indent=2) + \"\\n\")\n",
    "    logging.info(f\"Logged '{stage}' data to logs/{ctx.run_id}.log\")\n",
    "\n",
    "def parse_json_from_response(text: str) -> Dict[str, Any]:\n",
    "    match = re.search(r'```(?:json)?\\s*({.*?})\\s*```', text, re.S)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}')\n",
    "        if start != -1 and end != -1:\n",
    "            json_str = text[start:end+1]\n",
    "        else:\n",
    "            return {\"raw_text\": text}\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        logging.warning(f\"Failed to parse JSON from response: {text}\")\n",
    "        return {\"raw_text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mock-tools-md",
   "metadata": {},
   "source": [
    "### 2.1. Mock Tool Implementations\n",
    "\n",
    "In a real-world scenario, these functions would make API calls to internal databases, chemical suppliers, or literature search engines. For this notebook, we mock them to simulate their behavior and make the example self-contained and runnable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mock-tools-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "MOCK_CHEMICALS = {\n",
    "    \"Palladium acetate\": {\"cost_per_gram\": 85.50, \"hazards\": \"Irritant\"},\n",
    "    \"Triphenylphosphine\": {\"cost_per_gram\": 12.75, \"hazards\": \"Irritant\"},\n",
    "    \"Triethylamine\": {\"cost_per_gram\": 5.25, \"hazards\": \"Flammable, corrosive\"},\n",
    "    \"Sodium borohydride\": {\"cost_per_gram\": 8.90, \"hazards\": \"Flammable, water-reactive\"},\n",
    "    \"Dimethylformamide\": {\"cost_per_gram\": 3.15, \"hazards\": \"Reproductive toxin\"},\n",
    "    \"Palladium chloride\": {\"cost_per_gram\": 75.20, \"hazards\": \"Irritant, potential carcinogen\"},\n",
    "    \"Potassium carbonate\": {\"cost_per_gram\": 2.50, \"hazards\": \"Irritant\"},\n",
    "    \"Toluene\": {\"cost_per_gram\": 1.75, \"hazards\": \"Flammable, CNS depressant\"},\n",
    "}\n",
    "MOCK_OUTCOMES = {\"XYZ-13\": [{\"id\": \"exp-001\", \"yield\": 62.3, \"notes\": \"Yield decreased above 85C.\"}]}\n",
    "MOCK_LITERATURE = [{\"title\": \"Palladium-Catalyzed Cross-Coupling for XYZ Derivatives\", \"abstract\": \"Improved yields...\"}]\n",
    "\n",
    "def list_available_chemicals(): return {\"available_chemicals\": list(MOCK_CHEMICALS.keys())}\n",
    "def chem_lookup(chemical_name: str): return {\"properties\": MOCK_CHEMICALS.get(chemical_name, {})}\n",
    "def cost_estimator(reagents: List[Dict]):\n",
    "    # Ensure reagents is a list of dicts\n",
    "    if not isinstance(reagents, list):\n",
    "        return {\"status\": \"error\", \"message\": \"Expected a list of reagent dicts.\"}\n",
    "    total_cost = sum(\n",
    "        r.get('amount', r.get('quantity', 0)) * MOCK_CHEMICALS.get(r.get('name', ''), {}).get('cost_per_gram', 0)\n",
    "        for r in reagents if isinstance(r, dict)\n",
    "    )\n",
    "    return {\"total_cost\": round(total_cost, 2)}\n",
    "def outcome_db(compound: str): return {\"experiments\": MOCK_OUTCOMES.get(compound, [])}\n",
    "def literature_search(query: str): return {\"results\": MOCK_LITERATURE}\n",
    "\n",
    "TOOL_DISPATCHER = {\n",
    "    \"list_available_chemicals\": list_available_chemicals, \"chem_lookup\": chem_lookup,\n",
    "    \"cost_estimator\": cost_estimator, \"outcome_db\": outcome_db, \"literature_search\": literature_search\n",
    "}\n",
    "\n",
    "def get_tool_manifest():\n",
    "    # Abridged schemas for notebook clarity, matching original source\n",
    "    return [\n",
    "        {\"type\": \"function\", \"function\": {\"name\": \"list_available_chemicals\", \"description\": \"List all available chemicals.\"}},\n",
    "        {\"type\": \"function\", \"function\": {\"name\": \"chem_lookup\", \"description\": \"Look up chemical properties.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"chemical_name\": {\"type\": \"string\"}}, \"required\": [\"chemical_name\"]}}},\n",
    "        {\"type\": \"function\", \"function\": {\"name\": \"cost_estimator\", \"description\": \"Estimate experiment costs.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"reagents\": {\"type\": \"array\", \"items\": {\"type\": \"object\"}}}}}},\n",
    "        {\"type\": \"function\", \"function\": {\"name\": \"outcome_db\", \"description\": \"Query past experiment outcomes.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"compound\": {\"type\": \"string\"}}, \"required\": [\"compound\"]}}},\n",
    "        {\"type\": \"function\", \"function\": {\"name\": \"literature_search\", \"description\": \"Search scientific literature.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\"}}, \"required\": [\"query\"]}}}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "core-agent-logic-md",
   "metadata": {},
   "source": [
    "### 2.2. Core Agent Runner\n",
    "\n",
    "This central function, `call_openai`, is a direct adaptation from the source `agent_utils.py`. It handles the entire interaction loop with the language model, including making the initial request and recursively handling any tool calls the model makes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "core-agent-logic-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_openai(client: OpenAI, model: str, system: str, user: str, ctx: Context, step_name: str) -> Dict[str, Any]:\n",
    "    messages = [{\"role\": \"system\", \"content\": system}, {\"role\": \"user\", \"content\": user}]\n",
    "    \n",
    "    for i in range(5):  # Allow up to 5 tool call iterations\n",
    "        start_time = time.time()\n",
    "        logging.info(f\"Running step '{step_name}' with model '{model}' (Turn {i})...\")\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=messages, tools=get_tool_manifest(), tool_choice=\"auto\"\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        msg = response.choices[0].message\n",
    "        messages.append(msg)\n",
    "\n",
    "        p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "        metrics_log.append({\"step\": f\"{step_name}_{i}\", \"model\": model, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "\n",
    "        if not msg.tool_calls:\n",
    "            final_json = parse_json_from_response(msg.content)\n",
    "            log_json(step_name, final_json, ctx)\n",
    "            return final_json\n",
    "\n",
    "        logging.info(f\"Agent requested {len(msg.tool_calls)} tool call(s)...\")\n",
    "        for tool_call in msg.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            try:\n",
    "                args = json.loads(tool_call.function.arguments)\n",
    "                logging.info(f\"Calling tool: {function_name}({args})\")\n",
    "                result = TOOL_DISPATCHER[function_name](**args)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Tool call failed for {function_name}: {e}\")\n",
    "                result = {\"status\": \"error\", \"message\": str(e)}\n",
    "            \n",
    "            messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": json.dumps(result)})\n",
    "            \n",
    "    return {\"error\": \"Exceeded maximum tool call limit.\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-pipeline-md",
   "metadata": {},
   "source": [
    "## 3. Running the Co-Scientist Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "step1-input-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Starting Run ID: `83a87bd3` for compound: **XYZ-13**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 3.1: Scientist Input & Context Initialization\n",
    "user_input = {\n",
    "    \"compound\": \"XYZ-13\",\n",
    "    \"goal\": \"Improve synthesis yield by 15%\",\n",
    "    \"budget\": 15000,\n",
    "    \"time_h\": 48,\n",
    "    \"previous\": \"Prior attempts failed at high temp; explore potential catalyst effects.\",\n",
    "    \"client\": client\n",
    "}\n",
    "ctx = Context(**user_input)\n",
    "\n",
    "display(Markdown(f\"### Starting Run ID: `{ctx.run_id}` for compound: **{ctx.compound}**\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "step2-ideation-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:51:37,331 - INFO - --- Starting Ideation Phase ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ecd63a13ed8457eb4346a7f3aacb42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Ideation Agents:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:51:37,364 - INFO - Running step 'ideation_hypothesis_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 0)...\n",
      "2025-08-17 22:51:41,565 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:51:41,597 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:51:41,597 - INFO - Calling tool: list_available_chemicals({})\n",
      "2025-08-17 22:51:41,597 - INFO - Running step 'ideation_hypothesis_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 1)...\n",
      "2025-08-17 22:51:43,520 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:51:43,520 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:51:43,520 - INFO - Calling tool: outcome_db({'compound': 'XYZ-13'})\n",
      "2025-08-17 22:51:43,520 - INFO - Running step 'ideation_hypothesis_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 2)...\n",
      "2025-08-17 22:51:45,259 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:51:45,272 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:51:45,276 - INFO - Calling tool: literature_search({'query': 'catalysts for XYZ-13 synthesis'})\n",
      "2025-08-17 22:51:45,277 - INFO - Running step 'ideation_hypothesis_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 3)...\n",
      "2025-08-17 22:51:47,716 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:51:47,716 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:51:47,732 - INFO - Calling tool: chem_lookup({'chemical_name': 'Palladium acetate'})\n",
      "2025-08-17 22:51:47,732 - INFO - Running step 'ideation_hypothesis_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 4)...\n",
      "2025-08-17 22:51:50,411 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:51:50,411 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:51:50,416 - INFO - Calling tool: cost_estimator({'reagents': [{'chemical_name': 'Palladium acetate', 'quantity': 100}, {'chemical_name': 'Triphenylphosphine', 'quantity': 50}]})\n",
      "2025-08-17 22:51:50,419 - INFO - Running step 'ideation_protocol_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 0)...\n",
      "2025-08-17 22:51:53,525 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:51:53,531 - INFO - Agent requested 3 tool call(s)...\n",
      "2025-08-17 22:51:53,533 - INFO - Calling tool: outcome_db({'compound': 'XYZ-13'})\n",
      "2025-08-17 22:51:53,533 - INFO - Calling tool: list_available_chemicals({})\n",
      "2025-08-17 22:51:53,533 - INFO - Calling tool: literature_search({'query': 'catalyst effects on XYZ-13 synthesis'})\n",
      "2025-08-17 22:51:53,540 - INFO - Running step 'ideation_protocol_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 1)...\n",
      "2025-08-17 22:52:01,384 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:52:01,757 - INFO - Logged 'ideation_protocol_agent' data to logs/83a87bd3.log\n",
      "2025-08-17 22:52:01,758 - INFO - Running step 'ideation_resource_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 0)...\n",
      "2025-08-17 22:52:03,993 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:52:04,009 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:52:04,011 - INFO - Calling tool: outcome_db({'compound': 'XYZ-13'})\n",
      "2025-08-17 22:52:04,013 - INFO - Running step 'ideation_resource_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 1)...\n",
      "2025-08-17 22:52:06,143 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:52:06,160 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:52:06,162 - INFO - Calling tool: list_available_chemicals({})\n",
      "2025-08-17 22:52:06,164 - INFO - Running step 'ideation_resource_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 2)...\n",
      "2025-08-17 22:52:09,533 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:52:09,539 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:52:09,541 - INFO - Calling tool: chem_lookup({'chemical_name': 'Palladium acetate'})\n",
      "2025-08-17 22:52:09,543 - INFO - Running step 'ideation_resource_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 3)...\n",
      "2025-08-17 22:52:12,293 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:52:12,293 - INFO - Agent requested 1 tool call(s)...\n",
      "2025-08-17 22:52:12,304 - INFO - Calling tool: cost_estimator({'reagents': [{'name': 'Palladium acetate', 'quantity': 10}, {'name': 'Triphenylphosphine', 'quantity': 20}, {'name': 'Dimethylformamide', 'quantity': 50}]})\n",
      "2025-08-17 22:52:12,306 - INFO - Running step 'ideation_resource_agent' with model 'Qwen/Qwen3-4B-fast' (Turn 4)...\n",
      "2025-08-17 22:52:16,087 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:52:16,095 - INFO - Logged 'ideation_resource_agent' data to logs/83a87bd3.log\n",
      "2025-08-17 22:52:16,105 - INFO - Logged 'ideation_done' data to logs/83a87bd3.log\n"
     ]
    }
   ],
   "source": [
    "# Step 3.2: Ideation with Parallel Agents\n",
    "ROLE_FOCUS = {\n",
    "    \"hypothesis_agent\": \"You are a pharmaceutical hypothesis specialist. Focus exclusively on analyzing the compound structure and research goals to generate testable hypotheses. Consider mechanism of action, binding affinity predictions, and potential off-target effects.\",\n",
    "    \"protocol_agent\": \"You are a laboratory protocol specialist. Design experimental procedures that will effectively test the provided hypothesis. Focus on experimental conditions, controls, and measurement techniques.\",\n",
    "    \"resource_agent\": \"You are a laboratory resource optimization specialist. Review the proposed protocol and optimize for efficiency. Identify opportunities to reduce reagent use, equipment time, and overall costs while maintaining scientific validity.\"\n",
    "}\n",
    "\n",
    "# This two-part prompt construction mirrors the original source exactly\n",
    "IDEATION_PROMPT = \"\"\"You are a pharmaceutical {role} specialist. Your goal is to {goal} for compound {compound}.\n",
    "Constraints:\\n- Budget: ${budget}\\n- Approved reagents only\\n- Complete within {time_h} hours\\n- Previous attempts: {previous}\n",
    "Respond with structured JSON describing your protocol.\"\"\"\n",
    "\n",
    "IDEATION_PROMPT += \"\"\"\\nUse the following tools as appropriate:\n",
    "- Use the `list_available_chemicals` tool to get list of approved reagents.\n",
    "- Use the `chem_lookup` tool to verify properties of reagents mentioned.\n",
    "- Use the `cost_estimator` tool to calculate the approximate cost based on reagents and proposed steps.\n",
    "- Check the `outcome_db` for relevant prior experiments with {compound}\"\"\"\n",
    "\n",
    "def ideation(ctx: Context) -> List[Dict]:\n",
    "    logging.info(\"--- Starting Ideation Phase ---\")\n",
    "    ideas = []\n",
    "    for role, focus in tqdm(ROLE_FOCUS.items(), desc=\"Running Ideation Agents\"):\n",
    "        sys_prompt = IDEATION_PROMPT.format(role=role, focus=focus, **ctx.prompt_vars())\n",
    "        user_prompt = f\"Design a protocol to {ctx.goal} within ${ctx.budget}.\"\n",
    "        idea = call_openai(ctx.client, MODEL_IDEATE, sys_prompt, user_prompt, ctx, f\"ideation_{role}\")\n",
    "        ideas.append(idea)\n",
    "    log_json(\"ideation_done\", ideas, ctx)\n",
    "    return ideas\n",
    "\n",
    "generated_ideas = ideation(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "step3-tournament-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:52:16,144 - INFO - --- Starting Tournament Ranking Phase ---\n",
      "2025-08-17 22:52:16,149 - INFO - Running step 'tournament' with model 'Qwen/Qwen3-4B-fast' (Turn 0)...\n",
      "2025-08-17 22:52:21,202 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:52:21,217 - INFO - Logged 'tournament' data to logs/83a87bd3.log\n",
      "2025-08-17 22:52:21,217 - INFO - Tournament winner selected. Justification: Protocol B provides a detailed, actionable plan with specific reagents, budget, and scientific rationale. Protocol A is invalid due to the 'exceeded maximum tool call limit' error. Protocol B's steps (heterogeneous catalyst, microwave-assisted synthesis, base addition) align with established methods for yield improvement, and its cost breakdown is practical. While Protocol B carries moderate risk (e.g., catalyst efficiency), its feasibility and scientific justification make it the superior choice.\n",
      "2025-08-17 22:52:21,217 - INFO - Logged 'tournament' data to logs/83a87bd3.log\n"
     ]
    }
   ],
   "source": [
    "# Step 3.3: Tournament Ranking\n",
    "TOURNAMENT_PROMPT = \"\"\"Protocol A: {protocol_a}\n",
    "Protocol B: {protocol_b}\n",
    "\n",
    "Compare Protocol A and Protocol B for synthesizing {compound} aimed at {goal}. Score them on:\n",
    "1. Likelihood of achieving ≥ 15% yield increase.\n",
    "2. Practical feasibility (reagents, time).\n",
    "3. Estimated cost-efficiency (use tool if needed).\n",
    "4. Scientific novelty/risk.\n",
    "\n",
    "Return JSON {{\"winner\": \"A\"|\"B\", \"justification\": \"...\"}}.\"\"\"\n",
    "\n",
    "def tournament(protocols: List[Dict], ctx: Context) -> Dict:\n",
    "    logging.info(\"--- Starting Tournament Ranking Phase ---\")\n",
    "    if not protocols: return {}\n",
    "    if len(protocols) == 1: return protocols[0]\n",
    "    \n",
    "    protocol_a, protocol_b = protocols[0], protocols[1]\n",
    "    # Note: A real implementation would compare pairs in a bracket style\n",
    "    sys_prompt = TOURNAMENT_PROMPT.format(\n",
    "        protocol_a=json.dumps(protocol_a), protocol_b=json.dumps(protocol_b), **ctx.prompt_vars()\n",
    "    )\n",
    "    result = call_openai(ctx.client, MODEL_IDEATE, sys_prompt, \"Choose the winning protocol.\", ctx, \"tournament\")\n",
    "    winner = protocol_a if result.get('winner', 'A').upper() == 'A' else protocol_b\n",
    "    logging.info(f\"Tournament winner selected. Justification: {result.get('justification')}\")\n",
    "    log_json(\"tournament\", result, ctx)\n",
    "    return winner\n",
    "\n",
    "top_protocol = tournament(generated_ideas, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "step4-critique-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:52:34,306 - INFO - --- Starting Deep Critique Phase ---\n",
      "2025-08-17 22:52:34,314 - INFO - Running step 'critique' with model 'Qwen/Qwen3-235B-A22B' (Turn 0)...\n",
      "2025-08-17 22:53:28,893 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:53:28,907 - INFO - Logged 'critique' data to logs/83a87bd3.log\n",
      "2025-08-17 22:53:28,911 - INFO - Critique complete. Recommendation: go\n"
     ]
    }
   ],
   "source": [
    "# Step 3.4: Deep Critique & Synthesis\n",
    "CRITIQUE_PROMPT = \"\"\"You are a senior researcher reviewing a proposed synthesis protocol \n",
    "for {compound} aiming for {goal}, budget ${budget} using approved reagents. Review the protocol below rigorously:\n",
    "1. Identify scientific flaws or methodological weaknesses.\n",
    "2. Assess safety risks and budget compliance (use `cost_estimator` tool if needed).\n",
    "3. Check for consistency with prior `outcome_db` results if relevant.\n",
    "4. Suggest concrete improvements or rewrite sections if necessary.\n",
    "5. Provide a final go/no-go recommendation.\n",
    "\n",
    "Return JSON {{\"revised_protocol\": ..., \"critique\": \"...\", \"recommendation\": \"go|no-go\"}}.\"\"\"\n",
    "\n",
    "def critique(protocol: Dict, ctx: Context) -> Dict:\n",
    "    logging.info(\"--- Starting Deep Critique Phase ---\")\n",
    "    sys_prompt = CRITIQUE_PROMPT.format(**ctx.prompt_vars())\n",
    "    user_prompt = f\"Protocol to Review:\\n{json.dumps(protocol)}\"\n",
    "    result = call_openai(ctx.client, MODEL_CRITIQUE, sys_prompt, user_prompt, ctx, \"critique\")\n",
    "    logging.info(f\"Critique complete. Recommendation: {result.get('recommendation')}\")\n",
    "    ctx.critique_recommendation = result.get('recommendation', 'N/A')\n",
    "    return result.get(\"revised_protocol\", protocol)\n",
    "\n",
    "critiqued_protocol = critique(top_protocol, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "step5-safety-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:54:17,329 - INFO - --- Starting Safety Check Phase ---\n",
      "2025-08-17 22:54:17,329 - INFO - Running step 'safety_check' with model 'Qwen/Qwen3-14B' (Turn 0)...\n",
      "2025-08-17 22:54:39,070 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:54:39,080 - INFO - Agent requested 2 tool call(s)...\n",
      "2025-08-17 22:54:39,080 - INFO - Calling tool: chem_lookup({'chemical_name': 'N,N-Dimethylformamide'})\n",
      "2025-08-17 22:54:39,083 - INFO - Calling tool: chem_lookup({'chemical_name': 'Cesium carbonate'})\n",
      "2025-08-17 22:54:39,087 - INFO - Running step 'safety_check' with model 'Qwen/Qwen3-14B' (Turn 1)...\n",
      "2025-08-17 22:54:54,801 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:54:54,812 - INFO - Logged 'safety_check' data to logs/83a87bd3.log\n"
     ]
    }
   ],
   "source": [
    "# Step 3.5: Safety Check\n",
    "SAFETY_PROMPT = \"\"\"You are a lab‑safety specialist. \n",
    "Identify hazards, unsafe conditions, or compliance issues in this protocol for {compound}. \n",
    "Use `chem_lookup` tool if needed. Return JSON assessment.\"\"\"\n",
    "\n",
    "def safety(protocol: Dict, ctx: Context) -> Dict:\n",
    "    logging.info(\"--- Starting Safety Check Phase ---\")\n",
    "    sys_prompt = SAFETY_PROMPT.format(**ctx.prompt_vars())\n",
    "    user_prompt = json.dumps(protocol)\n",
    "    assessment = call_openai(ctx.client, MODEL_SAFETY, sys_prompt, user_prompt, ctx, \"safety_check\")\n",
    "    return {\"protocol\": protocol, \"safety_assessment\": assessment}\n",
    "\n",
    "final_package = safety(critiqued_protocol, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "step6-quality-eval-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:54:57,408 - INFO - --- Starting Automated Quality Evaluation ---\n",
      "2025-08-17 22:54:57,408 - INFO - Running step 'quality_evaluation' with model 'Qwen/Qwen3-235B-A22B' (Turn 0)...\n",
      "2025-08-17 22:55:23,784 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:55:23,798 - INFO - Logged 'quality_evaluation' data to logs/83a87bd3.log\n"
     ]
    }
   ],
   "source": [
    "# Step 3.6: [ENHANCEMENT] Automated Quality Evaluation\n",
    "def evaluate_protocol_quality(protocol: Dict, ctx: Context) -> Dict:\n",
    "    logging.info(\"--- Starting Automated Quality Evaluation ---\")\n",
    "    system_prompt = \"You are an expert panel of scientists evaluating a research protocol.\"\n",
    "    user_prompt = f\"\"\"Score the following protocol for the goal '{ctx.goal}' on a scale of 0.0 to 1.0 for each category.\n",
    "    - scientific_validity: How sound is the underlying science and hypothesis?\n",
    "    - feasibility: How practical is this to execute in a standard lab?\n",
    "    - innovation: How novel is this approach?\n",
    "    - cost_effectiveness: How well does it balance potential outcomes with cost?\n",
    "    - clarity_and_reproducibility: How clear and easy to follow are the instructions?\n",
    "    PROTOCOL: {json.dumps(protocol)}\n",
    "    Respond with a JSON object containing a score (float) and justification (string) for each of the five categories.\"\"\"\n",
    "    \n",
    "    quality_scores = call_openai(ctx.client, MODEL_EVALUATE, system_prompt, user_prompt, ctx, \"quality_evaluation\")\n",
    "    return quality_scores\n",
    "\n",
    "quality_assessment = evaluate_protocol_quality(final_package['protocol'], ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "step7-human-review-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:56:09,431 - INFO - --- Awaiting Human Review ---\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### PROTOCOL FOR HUMAN REVIEW"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Run ID:** `83a87bd3`"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Protocol Title:** N/A"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI Quality Assessment:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'scientific_validity': {'score': 0.85,\n",
       "  'justification': 'The protocol employs established principles of heterogeneous catalysis (Pd/C), ligand immobilization for recyclability, and base selection based on pKa. Microwave heating is a validated method for kinetic enhancement. The rationale aligns with mechanistic understanding of transition-metal catalysis.'},\n",
       " 'feasibility': {'score': 0.8,\n",
       "  'justification': 'Pd/C and Cs2CO3 are commercially available; microwave reactors with temperature/pressure control are standard in industrial labs. Scaling may require optimization of solvent volumes and catalyst loading, but no exotic equipment is required. Immobilized ligands may require specialized synthesis.'},\n",
       " 'innovation': {'score': 0.65,\n",
       "  'justification': \"Combines heterogeneous catalysis with microwave-assisted synthesis and base optimization. While not entirely novel individually, the specific integration for this compound's synthesis represents incremental innovation. Similar approaches have been reported in cross-coupling reactions (e.g., Buchwald-Hartwig).\"},\n",
       " 'cost_effectiveness': {'score': 0.45,\n",
       "  'justification': 'Estimated cost exceeds $9,600 primarily due to Pd/C (precious metal) and immobilized ligands. While reuse of catalyst could offset costs over time, no evidence of recyclability testing is provided. Cs2CO3 (6X cost of K2CO3) significantly impacts the budget without guaranteed yield improvement.'},\n",
       " 'clarity_and_reproducibility': {'score': 0.7,\n",
       "  'justification': 'Protocol specifies quantities, temperatures, and major equipment but lacks critical details: reaction time for microwave steps, catalyst recycling protocol, and yield baseline measurements. Solvent recovery and work-up procedures are undefined. Required equipment is generally identifiable.'}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**AI Safety Assessment:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'hazards': [{'type': 'chemical_hazard',\n",
       "   'description': 'Cesium carbonate (Cs2CO3) is a strong base and corrosive substance. Prolonged skin contact or inhalation may cause irritation or chemical burns. Requires PPE (gloves, goggles) and proper handling procedures.'},\n",
       "  {'type': 'toxicity',\n",
       "   'description': 'DMF is a suspected carcinogen and skin irritant. Inhalation or skin contact must be minimized using fume hoods, gloves, and lab coats.'},\n",
       "  {'type': 'fire_explosion',\n",
       "   'description': 'Pd/C (carbon support) dust may pose a fire hazard if not handled properly (e.g., under inert atmosphere or with dust controls).'},\n",
       "  {'type': 'equipment_safety',\n",
       "   'description': 'Microwave-assisted synthesis requires a reactor rated for pressure and thermal stability. Lack of specifications for equipment compatibility raises safety concerns.'},\n",
       "  {'type': 'waste_disposal',\n",
       "   'description': 'No protocols for disposal of Cs2CO3, DMF, or catalyst waste. Cesium compounds and organic solvents require specific hazardous waste handling.'}],\n",
       " 'unsafe_conditions': [{'type': 'inadequate_ppe',\n",
       "   'description': 'No mention of PPE (gloves, goggles) for handling strong bases or toxic solvents.'},\n",
       "  {'type': 'incomplete_procedure',\n",
       "   'description': 'Lack of details on pressure control parameters during microwave synthesis or immobilized PPh3 activation/storage.'}],\n",
       " 'compliance_issues': [{'type': 'osha_hazcom',\n",
       "   'description': 'Missing chemical safety data sheets (SDS) for Cs2CO3 and DMF in the protocol.'},\n",
       "  {'type': 'environmental',\n",
       "   'description': 'No waste disposal plan for cesium compounds or DMF, which require specialized treatment.'}]}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:56:24,140 - INFO - Protocol APPROVED by human reviewer.\n"
     ]
    }
   ],
   "source": [
    "# Step 3.7: Human Review\n",
    "def human_review(package: Dict, quality: Dict, ctx: Context) -> Dict:\n",
    "    logging.info(\"--- Awaiting Human Review ---\")\n",
    "    display(Markdown(\"### PROTOCOL FOR HUMAN REVIEW\"))\n",
    "    display(Markdown(f\"**Run ID:** `{ctx.run_id}`\"))\n",
    "    display(Markdown(f\"**Protocol Title:** {package['protocol'].get('protocol_title', 'N/A')}\"))\n",
    "    display(Markdown(\"**AI Quality Assessment:**\"))\n",
    "    display(quality)\n",
    "    display(Markdown(\"**AI Safety Assessment:**\"))\n",
    "    display(package['safety_assessment'])\n",
    "    \n",
    "    while True:\n",
    "        approval = input(\"\\nApprove protocol for execution? (yes/no): \").lower()\n",
    "        if approval in ['yes', 'y']:\n",
    "            logging.info(\"Protocol APPROVED by human reviewer.\")\n",
    "            return {\"approved\": True, \"final_protocol\": package['protocol']}\n",
    "        if approval in ['no', 'n']:\n",
    "            logging.info(\"Protocol REJECTED by human reviewer.\")\n",
    "            return {\"approved\": False, \"final_protocol\": package['protocol']}\n",
    "        print(\"Invalid input. Please enter 'yes' or 'no'.\")\n",
    "\n",
    "human_decision = human_review(final_package, quality_assessment, ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "step8-learning-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-17 22:56:35,420 - INFO - --- Starting Mock Execution and Learning Phase ---\n",
      "2025-08-17 22:56:35,427 - INFO - Running step 'analysis' with model 'Qwen/Qwen3-235B-A22B' (Turn 0)...\n",
      "2025-08-17 22:56:53,885 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-17 22:56:53,901 - INFO - Logged 'analysis' data to logs/83a87bd3.log\n",
      "2025-08-17 22:56:53,905 - INFO - Logged 'analysis' data to logs/83a87bd3.log\n",
      "2025-08-17 22:56:53,910 - INFO - Completed. Analysis summary written to output\\83a87bd3_summary.json\n"
     ]
    }
   ],
   "source": [
    "# Step 3.8: Execution & Learning (Mocked)\n",
    "ANALYSIS_PROMPT = \"\"\"You are a data analyst. \n",
    "Did the experiment achieve {goal}? Analyse factors, suggest improvements, and return structured JSON.\"\"\"\n",
    "\n",
    "def execute_and_analyse(decision: Dict, ctx: Context) -> Optional[Dict]:\n",
    "    if not decision['approved']:\n",
    "        logging.warning(\"Execution skipped as protocol was not approved.\")\n",
    "        return None\n",
    "    \n",
    "    logging.info(\"--- Starting Mock Execution and Learning Phase ---\")\n",
    "    mock_results = {\"yield_improvement\": 12.5, \"success\": False, \"actual_cost\": 12500, \"notes\": \"Yield improved but did not meet 15% target.\"}\n",
    "    \n",
    "    sys_prompt = ANALYSIS_PROMPT.format(**ctx.prompt_vars())\n",
    "    user_prompt = f\"\"\"The experiment has been run. The protocol was: {json.dumps(decision['final_protocol'])}. The results were: {json.dumps(mock_results)}. Analyze these results and return a structured JSON with a 'summary' and 'next_steps'.\"\"\"\n",
    "    \n",
    "    analysis = call_openai(ctx.client, MODEL_CRITIQUE, sys_prompt, user_prompt, ctx, \"analysis\")\n",
    "    log_json(\"analysis\", analysis, ctx)\n",
    "    \n",
    "    # Write final summary to file, as in original source\n",
    "    Path(\"output\").mkdir(exist_ok=True)\n",
    "    out_path = Path(\"output\") / f\"{ctx.run_id}_summary.json\"\n",
    "    out_path.write_text(json.dumps(analysis, indent=2))\n",
    "    logging.info(f\"Completed. Analysis summary written to {out_path}\")\n",
    "    return analysis\n",
    "\n",
    "learning_summary = execute_and_analyse(human_decision, ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-analysis-md-2",
   "metadata": {},
   "source": [
    "## 4. Final Analysis and Summary\n",
    "\n",
    "Finally, we'll consolidate all our metrics into two clear summaries. The first DataFrame provides a detailed breakdown of each API call, while the second offers a high-level summary of the entire run's performance and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "price-code-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prices_per_million_tokens = {\n",
    "    \"Qwen/Qwen3-4B-fast\": {\n",
    "        \"input\": 0.08,  # Example price in USD per 1M input tokens\n",
    "        \"output\": 0.24   # Example price in USD per 1M output tokens\n",
    "    },\n",
    "    \"Qwen/Qwen3-14B\": {\n",
    "        \"input\": 0.08,  # Example price\n",
    "        \"output\": 0.24   # Example price\n",
    "    },\n",
    "    \"Qwen/Qwen3-235B-A22B\": {\n",
    "        \"input\": 0.20,  # Example price for a large model\n",
    "        \"output\": 0.60   # Example price for a large model\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "per-step-dataframe-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Per-Step Performance and Cost Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>model</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>cost_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ideation_hypothesis_agent_0</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>4.232954</td>\n",
       "      <td>525</td>\n",
       "      <td>446</td>\n",
       "      <td>971</td>\n",
       "      <td>0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ideation_hypothesis_agent_1</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>1.923250</td>\n",
       "      <td>1039</td>\n",
       "      <td>262</td>\n",
       "      <td>1301</td>\n",
       "      <td>0.000146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ideation_hypothesis_agent_2</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>1.752038</td>\n",
       "      <td>1351</td>\n",
       "      <td>217</td>\n",
       "      <td>1568</td>\n",
       "      <td>0.000160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ideation_hypothesis_agent_3</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>2.438934</td>\n",
       "      <td>1613</td>\n",
       "      <td>223</td>\n",
       "      <td>1836</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ideation_hypothesis_agent_4</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>2.678942</td>\n",
       "      <td>1873</td>\n",
       "      <td>380</td>\n",
       "      <td>2253</td>\n",
       "      <td>0.000241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ideation_protocol_agent_0</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>3.111892</td>\n",
       "      <td>525</td>\n",
       "      <td>447</td>\n",
       "      <td>972</td>\n",
       "      <td>0.000149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ideation_protocol_agent_1</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>8.213036</td>\n",
       "      <td>1119</td>\n",
       "      <td>686</td>\n",
       "      <td>1805</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ideation_resource_agent_0</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>2.250857</td>\n",
       "      <td>525</td>\n",
       "      <td>302</td>\n",
       "      <td>827</td>\n",
       "      <td>0.000114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ideation_resource_agent_1</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>2.146295</td>\n",
       "      <td>877</td>\n",
       "      <td>296</td>\n",
       "      <td>1173</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ideation_resource_agent_2</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>3.374945</td>\n",
       "      <td>1241</td>\n",
       "      <td>483</td>\n",
       "      <td>1724</td>\n",
       "      <td>0.000215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ideation_resource_agent_3</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>2.750659</td>\n",
       "      <td>1761</td>\n",
       "      <td>396</td>\n",
       "      <td>2157</td>\n",
       "      <td>0.000236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ideation_resource_agent_4</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>3.781013</td>\n",
       "      <td>2181</td>\n",
       "      <td>549</td>\n",
       "      <td>2730</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>tournament_0</td>\n",
       "      <td>Qwen/Qwen3-4B-fast</td>\n",
       "      <td>5.064670</td>\n",
       "      <td>776</td>\n",
       "      <td>749</td>\n",
       "      <td>1525</td>\n",
       "      <td>0.000242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>critique_0</td>\n",
       "      <td>Qwen/Qwen3-235B-A22B</td>\n",
       "      <td>54.591720</td>\n",
       "      <td>803</td>\n",
       "      <td>2411</td>\n",
       "      <td>3214</td>\n",
       "      <td>0.001607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>safety_check_0</td>\n",
       "      <td>Qwen/Qwen3-14B</td>\n",
       "      <td>21.750463</td>\n",
       "      <td>746</td>\n",
       "      <td>1799</td>\n",
       "      <td>2545</td>\n",
       "      <td>0.000491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>safety_check_1</td>\n",
       "      <td>Qwen/Qwen3-14B</td>\n",
       "      <td>15.713580</td>\n",
       "      <td>2571</td>\n",
       "      <td>1314</td>\n",
       "      <td>3885</td>\n",
       "      <td>0.000521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>quality_evaluation_0</td>\n",
       "      <td>Qwen/Qwen3-235B-A22B</td>\n",
       "      <td>26.387956</td>\n",
       "      <td>852</td>\n",
       "      <td>990</td>\n",
       "      <td>1842</td>\n",
       "      <td>0.000764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>analysis_0</td>\n",
       "      <td>Qwen/Qwen3-235B-A22B</td>\n",
       "      <td>18.472378</td>\n",
       "      <td>815</td>\n",
       "      <td>860</td>\n",
       "      <td>1675</td>\n",
       "      <td>0.000679</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           step                 model  latency_s  \\\n",
       "0   ideation_hypothesis_agent_0    Qwen/Qwen3-4B-fast   4.232954   \n",
       "1   ideation_hypothesis_agent_1    Qwen/Qwen3-4B-fast   1.923250   \n",
       "2   ideation_hypothesis_agent_2    Qwen/Qwen3-4B-fast   1.752038   \n",
       "3   ideation_hypothesis_agent_3    Qwen/Qwen3-4B-fast   2.438934   \n",
       "4   ideation_hypothesis_agent_4    Qwen/Qwen3-4B-fast   2.678942   \n",
       "5     ideation_protocol_agent_0    Qwen/Qwen3-4B-fast   3.111892   \n",
       "6     ideation_protocol_agent_1    Qwen/Qwen3-4B-fast   8.213036   \n",
       "7     ideation_resource_agent_0    Qwen/Qwen3-4B-fast   2.250857   \n",
       "8     ideation_resource_agent_1    Qwen/Qwen3-4B-fast   2.146295   \n",
       "9     ideation_resource_agent_2    Qwen/Qwen3-4B-fast   3.374945   \n",
       "10    ideation_resource_agent_3    Qwen/Qwen3-4B-fast   2.750659   \n",
       "11    ideation_resource_agent_4    Qwen/Qwen3-4B-fast   3.781013   \n",
       "12                 tournament_0    Qwen/Qwen3-4B-fast   5.064670   \n",
       "13                   critique_0  Qwen/Qwen3-235B-A22B  54.591720   \n",
       "14               safety_check_0        Qwen/Qwen3-14B  21.750463   \n",
       "15               safety_check_1        Qwen/Qwen3-14B  15.713580   \n",
       "16         quality_evaluation_0  Qwen/Qwen3-235B-A22B  26.387956   \n",
       "17                   analysis_0  Qwen/Qwen3-235B-A22B  18.472378   \n",
       "\n",
       "    prompt_tokens  completion_tokens  total_tokens  cost_usd  \n",
       "0             525                446           971  0.000149  \n",
       "1            1039                262          1301  0.000146  \n",
       "2            1351                217          1568  0.000160  \n",
       "3            1613                223          1836  0.000183  \n",
       "4            1873                380          2253  0.000241  \n",
       "5             525                447           972  0.000149  \n",
       "6            1119                686          1805  0.000254  \n",
       "7             525                302           827  0.000114  \n",
       "8             877                296          1173  0.000141  \n",
       "9            1241                483          1724  0.000215  \n",
       "10           1761                396          2157  0.000236  \n",
       "11           2181                549          2730  0.000306  \n",
       "12            776                749          1525  0.000242  \n",
       "13            803               2411          3214  0.001607  \n",
       "14            746               1799          2545  0.000491  \n",
       "15           2571               1314          3885  0.000521  \n",
       "16            852                990          1842  0.000764  \n",
       "17            815                860          1675  0.000679  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.1 Per-Step Operational Metrics\n",
    "if metrics_log:\n",
    "    df_metrics = pd.DataFrame(metrics_log)\n",
    "\n",
    "    def calculate_cost(row):\n",
    "        prices = model_prices_per_million_tokens.get(row['model'], {\"input\": 0, \"output\": 0})\n",
    "        return (row['prompt_tokens'] / 1_000_000) * prices['input'] + (row['completion_tokens'] / 1_000_000) * prices['output']\n",
    "\n",
    "    df_metrics['cost_usd'] = df_metrics.apply(calculate_cost, axis=1)\n",
    "    \n",
    "    display(Markdown(\"### Per-Step Performance and Cost Analysis\"))\n",
    "    display(df_metrics)\n",
    "else:\n",
    "    logging.info(\"No metrics were logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "summary-dataframe-code-2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Final Run Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Run ID</th>\n",
       "      <td>83a87bd3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Compound</th>\n",
       "      <td>XYZ-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Latency (s)</th>\n",
       "      <td>180.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Cost (USD)</th>\n",
       "      <td>$0.006600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Tokens</th>\n",
       "      <td>34,003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Critique Recommendation</th>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Human Decision</th>\n",
       "      <td>Approved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--- Quality Scores (AI) ---</th>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Scientific Validity</th>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Feasibility</th>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Innovation</th>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cost-Effectiveness</th>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Clarity &amp; Reproducibility</th>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>**Overall Quality Score**</th>\n",
       "      <td>**0.69**</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Value\n",
       "Metric                                \n",
       "Run ID                        83a87bd3\n",
       "Compound                        XYZ-13\n",
       "Total Latency (s)               180.64\n",
       "Total Cost (USD)             $0.006600\n",
       "Total Tokens                    34,003\n",
       "Critique Recommendation             go\n",
       "Human Decision                Approved\n",
       "--- Quality Scores (AI) ---        ---\n",
       "Scientific Validity               0.85\n",
       "Feasibility                       0.80\n",
       "Innovation                        0.65\n",
       "Cost-Effectiveness                0.45\n",
       "Clarity & Reproducibility         0.70\n",
       "**Overall Quality Score**     **0.69**"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4.2 Final Run Summary\n",
    "if metrics_log:\n",
    "    def get_score(assessment, key):\n",
    "        if isinstance(assessment, dict) and key in assessment and isinstance(assessment[key], dict):\n",
    "            return assessment[key].get('score', 0.0)\n",
    "        return 0.0\n",
    "\n",
    "    scores = {\n",
    "        'validity': get_score(quality_assessment, 'scientific_validity'),\n",
    "        'feasibility': get_score(quality_assessment, 'feasibility'),\n",
    "        'innovation': get_score(quality_assessment, 'innovation'),\n",
    "        'cost_effect': get_score(quality_assessment, 'cost_effectiveness'),\n",
    "        'clarity': get_score(quality_assessment, 'clarity_and_reproducibility')\n",
    "    }\n",
    "    avg_quality = sum(scores.values()) / len(scores) if scores else 0.0\n",
    "\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Run ID',\n",
    "            'Compound',\n",
    "            'Total Latency (s)',\n",
    "            'Total Cost (USD)',\n",
    "            'Total Tokens',\n",
    "            'Critique Recommendation',\n",
    "            'Human Decision',\n",
    "            '--- Quality Scores (AI) ---',\n",
    "            'Scientific Validity',\n",
    "            'Feasibility',\n",
    "            'Innovation',\n",
    "            'Cost-Effectiveness',\n",
    "            'Clarity & Reproducibility',\n",
    "            '**Overall Quality Score**'\n",
    "        ],\n",
    "        'Value': [\n",
    "            ctx.run_id,\n",
    "            ctx.compound,\n",
    "            f\"{df_metrics['latency_s'].sum():.2f}\",\n",
    "            f\"${df_metrics['cost_usd'].sum():.6f}\",\n",
    "            f\"{df_metrics['total_tokens'].sum():,}\",\n",
    "            ctx.critique_recommendation,\n",
    "            'Approved' if human_decision.get('approved') else 'Rejected',\n",
    "            '---',\n",
    "            f\"{scores['validity']:.2f}\",\n",
    "            f\"{scores['feasibility']:.2f}\",\n",
    "            f\"{scores['innovation']:.2f}\",\n",
    "            f\"{scores['cost_effect']:.2f}\",\n",
    "            f\"{scores['clarity']:.2f}\",\n",
    "            f\"**{avg_quality:.2f}**\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data).set_index('Metric')\n",
    "    \n",
    "    display(Markdown(\"### Final Run Summary\"))\n",
    "    display(df_summary)\n",
    "else:\n",
    "    logging.info(\"Cannot generate summary as no metrics were logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-cell-2",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "This notebook has demonstrated a complete, end-to-end implementation of the AI Co-Scientist workflow using customizable open-source language models. By preserving the original's sophisticated multi-agent architecture and enhancing it with automated quality scoring and comprehensive summary reporting, we have created a powerful and observable system for complex R&D tasks.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1.  **The Escalation of Intelligence Pattern is Effective:** Using cheap, fast models for broad ideation and powerful, expensive models for deep critique is a cost-effective strategy for high-quality results.\n",
    "2.  **Automated Evaluation Adds a Layer of Confidence:** Adding an LLM-based quality assessment step provides a quantifiable measure of the AI's output before human review, aiding decision-making and long-term tracking.\n",
    "3.  **Observability is Paramount:** Consolidating operational and qualitative metrics into a final summary is critical for understanding system performance, debugging issues, and demonstrating value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-model-selection (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
