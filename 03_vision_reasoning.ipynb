{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Vision and Reasoning\n",
    "\n",
    "This notebook implements an advanced, two-stage pipeline for digitizing and validating hand-filled forms using open-source vision and language models. It is a high-fidelity recreation of the architecture from OpenAI's Model Selection Guide, enhanced with a detailed, automated evaluation stage for production-grade observability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architecture-discussion",
   "metadata": {},
   "source": [
    "## Architecture Overview and Enhancements\n",
    "\n",
    "We adopt the original's robust two-stage workflow:\n",
    "\n",
    "**Stage 1: Vision-based OCR Extraction**\n",
    "*   A powerful vision-capable LLM performs an initial, high-accuracy OCR on the form image.\n",
    "*   The key principle is to extract text literally and preserve ambiguity (e.g., `'jsmithl@gmail.com OR jsmith1@gmail.com'`). It does not guess or infer missing information.\n",
    "\n",
    "**Stage 2: Reasoning and Refinement**\n",
    "*   A cost-effective reasoning LLM takes the JSON from Stage 1 as input.\n",
    "*   Using a suite of tools (like web search and email validation), it resolves ambiguities, fills in missing information (like zip codes), and ensures the data conforms to the final schema.\n",
    "\n",
    "**Our Enhancements:**\n",
    "*   **Automated Quality Evaluation Agent:** A new, final stage where an evaluation LLM compares the output of Stage 1 and Stage 2 to provide granular scores for accuracy, inference quality, and completeness.\n",
    "*   **Comprehensive Final Summary:** A final DataFrame that consolidates all operational metrics (cost, latency) and our new quality scores, providing a holistic view of the entire process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-config",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "install-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU openai pandas tqdm pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-and-client",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "import logging\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display, Markdown, Image\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "API_KEY = \"API_KEY\"  # Replace with your actual API key\n",
    "BASE_URL = \"https://api.studio.nebius.com/v1/\"\n",
    "\n",
    "# NOTE: The chosen vision model MUST support image inputs (e.g., multimodal models like LLaVA, CogVLM, etc.)\n",
    "MODEL_VISION = \"google/gemma-3-27b-it\" # A powerful multimodal model for OCR\n",
    "MODEL_REASONING = \"Qwen/Qwen3-14B\"      # A smaller, cheaper model for refinement\n",
    "MODEL_EVALUATE = \"Qwen/Qwen3-235B-A22B\"    # A powerful model for accurate evaluation\n",
    "\n",
    "client = OpenAI(api_key=API_KEY, base_url=BASE_URL)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "metrics_log = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "utils-md",
   "metadata": {},
   "source": [
    "### 1.1. Utilities and Data Structures\n",
    "\n",
    "We start by defining our target data schema using Pydantic, which ensures that the LLM's output is structured correctly. We also include a robust helper function for parsing JSON from model responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pydantic-and-utils-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PersonContact(BaseModel):\n",
    "    name: str\n",
    "    home_phone: str\n",
    "    work_phone: str\n",
    "    cell_phone: str\n",
    "    email: str\n",
    "\n",
    "class Address(BaseModel):\n",
    "    street: str\n",
    "    city: str\n",
    "    state: str\n",
    "    zip: str\n",
    "    county: str\n",
    "\n",
    "class DwellingDetails(BaseModel):\n",
    "    coverage_a_limit: str\n",
    "    companion_policy_expiration_date: str\n",
    "    occupancy_of_dwelling: str\n",
    "    type_of_policy: str\n",
    "    unrepaired_structural_damage: bool\n",
    "    construction_type: str\n",
    "    roof_type: str\n",
    "    foundation_type: str\n",
    "    has_post_and_pier_or_post_and_beam_foundation: bool\n",
    "    cripple_walls: bool\n",
    "    number_of_stories: str\n",
    "    living_space_over_garage: bool\n",
    "    number_of_chimneys: str\n",
    "    square_footage: str\n",
    "    year_of_construction: str\n",
    "    anchored_to_foundation: bool\n",
    "    water_heater_secured: bool\n",
    "\n",
    "class InsuranceFormData(BaseModel):\n",
    "    applicant: PersonContact\n",
    "    co_applicant: PersonContact\n",
    "    risk_address: Address\n",
    "    mailing_address_if_different_than_risk_address: Address\n",
    "    participating_insurer: str\n",
    "    companion_policy_number: str\n",
    "    dwelling_details: DwellingDetails\n",
    "    effective_date: str\n",
    "    expiration_date: str\n",
    "\n",
    "def parse_json_from_response(text: str) -> Dict[str, Any]:\n",
    "    match = re.search(r'```(?:json)?\\s*({.*?})\\s*```', text, re.S)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}')\n",
    "        if start != -1 and end != -1:\n",
    "            json_str = text[start:end+1]\n",
    "        else:\n",
    "            return {\"raw_text\": text}\n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        logging.warning(f\"Failed to parse JSON from response: {text}\")\n",
    "        return {\"raw_text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage1-md",
   "metadata": {},
   "source": [
    "## 2. Stage 1: Vision-based OCR Extraction\n",
    "\n",
    "In this stage, we send the image of the insurance form to our powerful multimodal LLM. The prompt is carefully crafted to instruct the model to perform a literal transcription, explicitly preserving any ambiguities it finds and leaving blank fields empty. This prevents the model from making premature assumptions and provides a clean, raw extraction for the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "stage1-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://drive.usercontent.google.com/download?id=1-tZ526AW3mX1qthvgi8spaaxxeqFG5_6\" width=\"600\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 09:28:25,759 - INFO - --- Starting Stage 1: Vision-based OCR Extraction ---\n",
      "C:\\Users\\faree\\AppData\\Local\\Temp\\ipykernel_14240\\3772573333.py:7: PydanticDeprecatedSince20: The `schema_json` method is deprecated; use `model_json_schema` and json.dumps instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  {InsuranceFormData.schema_json(indent=2)}\n",
      "2025-08-18 09:28:37,501 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Stage 1 OCR Output (with ambiguities):"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'applicant': {'name': 'Smith, James L',\n",
       "  'home_phone': '510 331 5555',\n",
       "  'work_phone': '',\n",
       "  'cell_phone': '510 212 5555',\n",
       "  'email': 'jsmith@gmail.com'},\n",
       " 'co_applicant': {'name': 'Roberts, Jesse T',\n",
       "  'home_phone': '510 331 5555',\n",
       "  'work_phone': '415 626 5555',\n",
       "  'cell_phone': '',\n",
       "  'email': 'jrobertsjr@gmail.com'},\n",
       " 'risk_address': {'street': '855 Brannan St',\n",
       "  'city': 'San Francisco',\n",
       "  'state': 'CA',\n",
       "  'zip': '',\n",
       "  'county': ''},\n",
       " 'mailing_address_if_different_than_risk_address': {'street': '',\n",
       "  'city': '',\n",
       "  'state': '',\n",
       "  'zip': '',\n",
       "  'county': ''},\n",
       " 'participating_insurer': 'Agne Insurance Co',\n",
       " 'companion_policy_number': '8126918',\n",
       " 'dwelling_details': {'coverage_a_limit': '$900,000',\n",
       "  'companion_policy_expiration_date': '5/31/27',\n",
       "  'occupancy_of_dwelling': 'Owner',\n",
       "  'type_of_policy': 'Homeowners',\n",
       "  'unrepaired_structural_damage': False,\n",
       "  'construction_type': 'Wood Shake OR Other',\n",
       "  'roof_type': 'Wood Shake OR Other',\n",
       "  'foundation_type': 'Slab',\n",
       "  'has_post_and_pier_or_post_and_beam_foundation': False,\n",
       "  'cripple_walls': False,\n",
       "  'number_of_stories': '2',\n",
       "  'living_space_over_garage': False,\n",
       "  'number_of_chimneys': '0',\n",
       "  'square_footage': '2605',\n",
       "  'year_of_construction': '1920',\n",
       "  'anchored_to_foundation': True,\n",
       "  'water_heater_secured': True},\n",
       " 'effective_date': '3/31/25',\n",
       " 'expiration_date': '5/31/27'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FORM_IMAGE_URL = \"https://drive.usercontent.google.com/download?id=1-tZ526AW3mX1qthvgi8spaaxxeqFG5_6\"\n",
    "display(Image(url=FORM_IMAGE_URL, width=600))\n",
    "\n",
    "def run_ocr_stage(image_url: str) -> Dict[str, Any]:\n",
    "    logging.info(\"--- Starting Stage 1: Vision-based OCR Extraction ---\")\n",
    "    ocr_prompt = f\"\"\"You are an expert at processing insurance forms. OCR the data from the user-provided image into a structured JSON format that conforms to the following Pydantic schema:\n",
    "    {InsuranceFormData.schema_json(indent=2)}\n",
    "\n",
    "    IMPORTANT INSTRUCTIONS:\n",
    "    1. Fill out the fields as literally and exactly as possible from the image.\n",
    "    2. If a field is blank on the form, leave the corresponding JSON string field empty ('').\n",
    "    3. If a written character is ambiguous (e.g., could be 'l' or '1', 'o' or '0'), include all possibilities in the string, separated by ' OR '. This is especially important for email addresses.\n",
    "    4. Do NOT infer or guess any information that is not explicitly written on the form.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": ocr_prompt},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": image_url, \"detail\": \"high\"}}\n",
    "        ]}\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=MODEL_VISION, messages=messages, temperature=0.0)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"ocr_extraction\", \"model\": MODEL_VISION, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "    \n",
    "    return parse_json_from_response(response_text)\n",
    "\n",
    "stage1_output = run_ocr_stage(FORM_IMAGE_URL)\n",
    "display(Markdown(\"### Stage 1 OCR Output (with ambiguities):\"))\n",
    "display(stage1_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage2-md",
   "metadata": {},
   "source": [
    "## 3. Stage 2: Reasoning and Refinement\n",
    "\n",
    "Now, we pass the raw, ambiguous JSON from Stage 1 to a second, more specialized reasoning agent. This agent's job is to act as a data validation expert. It uses a set of tools to resolve the ambiguities and fill in the missing information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "stage2-tools-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Tools for Stage 2\n",
    "def validate_email(email: str) -> bool:\n",
    "    \"\"\"Mock function to validate an email. In a real system, this could check a database or a validation service.\"\"\"\n",
    "    logging.info(f\"VALIDATING EMAIL: {email}\")\n",
    "    # For this demo, we'll pretend only 'jsmithl@gmail.com' is the valid one.\n",
    "    return email == \"jsmithl@gmail.com\"\n",
    "\n",
    "def search_web(query: str) -> str:\n",
    "    \"\"\"Mock function for web search. In a real system, this would call a search API.\"\"\"\n",
    "    logging.info(f\"SEARCHING WEB FOR: {query}\")\n",
    "    if \"855 Brannan St\" in query:\n",
    "        return \"The full address is 855 Brannan St, San Francisco, CA 94103. This is in San Francisco County.\"\n",
    "    return \"No information found.\"\n",
    "\n",
    "TOOL_DISPATCHER = {\"validate_email\": validate_email, \"search_web\": search_web}\n",
    "\n",
    "def get_tool_manifest():\n",
    "    return [\n",
    "        {\"type\": \"function\", \"function\": {\"name\": \"validate_email\", \"description\": \"Check if an email address is valid and exists.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"email\": {\"type\": \"string\"}}, \"required\": [\"email\"]}}},\n",
    "        {\"type\": \"function\", \"function\": {\"name\": \"search_web\", \"description\": \"Perform a web search to find missing information like zip codes or counties.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"query\": {\"type\": \"string\"}}, \"required\": [\"query\"]}}}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "stage2-runner-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 09:29:05,952 - INFO - --- Starting Stage 2: Reasoning and Refinement ---\n",
      "2025-08-18 09:29:15,788 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-18 09:29:15,811 - INFO - Refinement agent requested 1 tool call(s)...\n",
      "2025-08-18 09:29:15,811 - INFO - SEARCHING WEB FOR: 855 Brannan St San Francisco CA zip code\n",
      "2025-08-18 09:29:27,358 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-08-18 09:29:27,373 - INFO - Refinement agent has finished reasoning.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Stage 2 Refined Output:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'applicant': {'name': 'Smith, James L',\n",
       "  'home_phone': '510 331 5555',\n",
       "  'work_phone': '',\n",
       "  'cell_phone': '510 212 5555',\n",
       "  'email': 'jsmith@gmail.com'},\n",
       " 'co_applicant': {'name': 'Roberts, Jesse T',\n",
       "  'home_phone': '510 331 5555',\n",
       "  'work_phone': '415 626 5555',\n",
       "  'cell_phone': '',\n",
       "  'email': 'jrobertsjr@gmail.com'},\n",
       " 'risk_address': {'street': '855 Brannan St',\n",
       "  'city': 'San Francisco',\n",
       "  'state': 'CA',\n",
       "  'zip': '94103',\n",
       "  'county': 'San Francisco'},\n",
       " 'mailing_address_if_different_than_risk_address': {'street': '855 Brannan St',\n",
       "  'city': 'San Francisco',\n",
       "  'state': 'CA',\n",
       "  'zip': '94103',\n",
       "  'county': 'San Francisco'},\n",
       " 'participating_insurer': 'Agne Insurance Co',\n",
       " 'companion_policy_number': '8126918',\n",
       " 'dwelling_details': {'coverage_a_limit': '$900,000',\n",
       "  'companion_policy_expiration_date': '5/31/27',\n",
       "  'occupancy_of_dwelling': 'Owner',\n",
       "  'type_of_policy': 'Homeowners',\n",
       "  'unrepaired_structural_damage': False,\n",
       "  'construction_type': 'Wood Shake OR Other',\n",
       "  'roof_type': 'Wood Shake OR Other',\n",
       "  'foundation_type': 'Slab',\n",
       "  'has_post_and_pier_or_post_and_beam_foundation': False,\n",
       "  'cripple_walls': False,\n",
       "  'number_of_stories': '2',\n",
       "  'living_space_over_garage': False,\n",
       "  'number_of_chimneys': '0',\n",
       "  'square_footage': '2605',\n",
       "  'year_of_construction': '1920',\n",
       "  'anchored_to_foundation': True,\n",
       "  'water_heater_secured': True},\n",
       " 'effective_date': '3/31/25',\n",
       " 'expiration_date': '5/31/27'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_refinement_stage(ocr_data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    logging.info(\"--- Starting Stage 2: Reasoning and Refinement ---\")\n",
    "    refinement_prompt = \"\"\"You are a data validation expert. You have been given a raw JSON output from an OCR system. Your job is to clean, validate, and complete it.\n",
    "    1. Examine each field for ambiguities (e.g., text containing ' OR '). Use the provided tools to resolve them to a single, correct value.\n",
    "    2. Identify any missing information (e.g., empty strings for `zip` or `county`). Use tools to find the correct information.\n",
    "    3. If the mailing address is empty, assume it is the same as the risk address and fill it in accordingly.\n",
    "    4. Return the final, cleaned, and validated JSON object that conforms perfectly to the Pydantic schema. Do not include the schema in your response.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": refinement_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"Here is the raw OCR data. Please refine it:\\n\\n{json.dumps(ocr_data, indent=2)}\"}\n",
    "    ]\n",
    "    \n",
    "    # This loop handles the conversation with the tool-using agent\n",
    "    for i in range(5): # Max 5 turns of conversation\n",
    "        start_time = time.time()\n",
    "        response = client.chat.completions.create(model=MODEL_REASONING, messages=messages, tools=get_tool_manifest(), tool_choice=\"auto\")\n",
    "        latency = time.time() - start_time\n",
    "        msg = response.choices[0].message\n",
    "        messages.append(msg)\n",
    "        \n",
    "        p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "        metrics_log.append({\"step\": f\"refinement_turn_{i}\", \"model\": MODEL_REASONING, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "        \n",
    "        if not msg.tool_calls:\n",
    "            logging.info(\"Refinement agent has finished reasoning.\")\n",
    "            return parse_json_from_response(msg.content)\n",
    "\n",
    "        logging.info(f\"Refinement agent requested {len(msg.tool_calls)} tool call(s)...\")\n",
    "        for tool_call in msg.tool_calls:\n",
    "            function_name = tool_call.function.name\n",
    "            try:\n",
    "                args = json.loads(tool_call.function.arguments)\n",
    "                result = TOOL_DISPATCHER[function_name](**args)\n",
    "                messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": json.dumps(result)})\n",
    "            except Exception as e:\n",
    "                messages.append({\"role\": \"tool\", \"tool_call_id\": tool_call.id, \"content\": json.dumps({\"error\": str(e)})})\n",
    "    \n",
    "    return {\"error\": \"Exceeded maximum tool call limit.\"}\n",
    "\n",
    "stage2_output = run_refinement_stage(stage1_output)\n",
    "display(Markdown(\"### Stage 2 Refined Output:\"))\n",
    "display(stage2_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stage3-md",
   "metadata": {},
   "source": [
    "## 4. Automated Quality Evaluation\n",
    "\n",
    "This is our enhanced evaluation step. We now use a powerful evaluation model to act as a quality assurance specialist. It compares the initial raw extraction with the final refined output to provide a detailed report on the system's performance for this specific form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stage3-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-18 09:29:30,181 - INFO - --- Starting Stage 3: Automated Quality Evaluation ---\n",
      "2025-08-18 09:31:42,917 - INFO - HTTP Request: POST https://api.studio.nebius.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Stage 3 Automated Quality Report:"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'field_accuracy_score': {'score': 0.9,\n",
       "  'justification': \"The final data accurately preserves all original information from the RAW OCR DATA. The only discrepancies are in the 'mailing_address_if_different_than_risk_address' fields, which were filled with the same values as the risk address despite the field name implying it should only be populated if different. The risk address zip and county were correctly inferred as '94103' and 'San Francisco', respectively.\"},\n",
       " 'inference_quality_score': {'score': 0.6,\n",
       "  'justification': 'The model successfully filled missing zip and county fields for the risk address with accurate data. However, the inference to populate the mailing address with identical values to the risk address is questionable, as the field definition implies it should remain blank if the addresses are the same. This creates potential inaccuracies in the mailing address section.'},\n",
       " 'completeness_score': {'score': 1.0,\n",
       "  'justification': 'All fields in the FINAL REFINED DATA are populated, achieving full completeness. Missing fields from the RAW OCR DATA (risk address zip/county and all mailing address fields) were filled.'},\n",
       " 'overall_confidence_score': {'score': 0.85,\n",
       "  'justification': 'The final data is largely accurate and complete, with correct inferences for the risk address zip/county. However, the decision to populate the mailing address fields with identical values to the risk address introduces ambiguity about its adherence to form conventions, slightly reducing confidence.'},\n",
       " 'fields_requiring_human_review': ['mailing_address_if_different_than_risk_address.street',\n",
       "  'mailing_address_if_different_than_risk_address.city',\n",
       "  'mailing_address_if_different_than_risk_address.state',\n",
       "  'mailing_address_if_different_than_risk_address.zip',\n",
       "  'mailing_address_if_different_than_risk_address.county']}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_extraction_quality(raw_data: Dict, final_data: Dict) -> Dict:\n",
    "    logging.info(\"--- Starting Stage 3: Automated Quality Evaluation ---\")\n",
    "    eval_prompt = f\"\"\"You are a Quality Assurance expert. Compare the 'RAW OCR DATA' with the 'FINAL REFINED DATA' and provide a detailed evaluation.\n",
    "\n",
    "    RAW OCR DATA:\n",
    "    {json.dumps(raw_data, indent=2)}\n",
    "\n",
    "    FINAL REFINED DATA:\n",
    "    {json.dumps(final_data, indent=2)}\n",
    "\n",
    "    Provide your evaluation as a JSON object with the following structure:\n",
    "    {{\n",
    "      \"field_accuracy_score\": {{ \"score\": float (0.0-1.0), \"justification\": \"Assess the overall accuracy of the final fields compared to the raw data.\" }},\n",
    "      \"inference_quality_score\": {{ \"score\": float (0.0-1.0), \"justification\": \"Specifically assess how well the model filled in MISSING or resolved AMBIGUOUS fields (e.g., zip code, email).\" }},\n",
    "      \"completeness_score\": {{ \"score\": float (0.0-1.0), \"justification\": \"Calculate the ratio of non-empty fields in the final data to the total number of fields.\" }},\n",
    "      \"overall_confidence_score\": {{ \"score\": float (0.0-1.0), \"justification\": \"Your holistic confidence in the final data's correctness.\" }},\n",
    "      \"fields_requiring_human_review\": [\"list\", \"of\", \"field_names\", \"that still seem uncertain or could not be verified\"]\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": eval_prompt}]\n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=MODEL_EVALUATE, messages=messages, temperature=0.0)\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"quality_evaluation\", \"model\": MODEL_EVALUATE, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "\n",
    "    return parse_json_from_response(response_text)\n",
    "\n",
    "quality_assessment = evaluate_extraction_quality(stage1_output, stage2_output)\n",
    "display(Markdown(\"### Stage 3 Automated Quality Report:\"))\n",
    "display(quality_assessment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-analysis-md",
   "metadata": {},
   "source": [
    "## 5. Final Analysis and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "price-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prices_per_million_tokens = {\n",
    "    \"google/gemma-3-27b-it\": {\n",
    "        \"input\": 0.10,  # Example price in USD per 1M input tokens\n",
    "        \"output\": 0.30   # Example price in USD per 1M output tokens\n",
    "    },\n",
    "    \"Qwen/Qwen3-14B\": {\n",
    "        \"input\": 0.08,  # Example price\n",
    "        \"output\": 0.24   # Example price\n",
    "    },\n",
    "    \"Qwen/Qwen3-235B-A22B\": {\n",
    "        \"input\": 0.20,  # Example price for a large model\n",
    "        \"output\": 0.60   # Example price for a large model\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "per-step-dataframe-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Per-Step Performance and Cost Analysis"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>model</th>\n",
       "      <th>latency_s</th>\n",
       "      <th>prompt_tokens</th>\n",
       "      <th>completion_tokens</th>\n",
       "      <th>total_tokens</th>\n",
       "      <th>cost_usd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ocr_extraction</td>\n",
       "      <td>google/gemma-3-27b-it</td>\n",
       "      <td>11.770877</td>\n",
       "      <td>1947</td>\n",
       "      <td>593</td>\n",
       "      <td>2540</td>\n",
       "      <td>0.000373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>refinement_turn_0</td>\n",
       "      <td>Qwen/Qwen3-14B</td>\n",
       "      <td>9.855705</td>\n",
       "      <td>882</td>\n",
       "      <td>771</td>\n",
       "      <td>1653</td>\n",
       "      <td>0.000256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>refinement_turn_1</td>\n",
       "      <td>Qwen/Qwen3-14B</td>\n",
       "      <td>11.558692</td>\n",
       "      <td>1696</td>\n",
       "      <td>942</td>\n",
       "      <td>2638</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>quality_evaluation</td>\n",
       "      <td>Qwen/Qwen3-235B-A22B</td>\n",
       "      <td>132.759243</td>\n",
       "      <td>1309</td>\n",
       "      <td>4236</td>\n",
       "      <td>5545</td>\n",
       "      <td>0.002803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 step                  model   latency_s  prompt_tokens  \\\n",
       "0      ocr_extraction  google/gemma-3-27b-it   11.770877           1947   \n",
       "1   refinement_turn_0         Qwen/Qwen3-14B    9.855705            882   \n",
       "2   refinement_turn_1         Qwen/Qwen3-14B   11.558692           1696   \n",
       "3  quality_evaluation   Qwen/Qwen3-235B-A22B  132.759243           1309   \n",
       "\n",
       "   completion_tokens  total_tokens  cost_usd  \n",
       "0                593          2540  0.000373  \n",
       "1                771          1653  0.000256  \n",
       "2                942          2638  0.000362  \n",
       "3               4236          5545  0.002803  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.2 Per-Step Operational Metrics\n",
    "if metrics_log:\n",
    "    df_metrics = pd.DataFrame(metrics_log)\n",
    "\n",
    "    def calculate_cost(row):\n",
    "        prices = model_prices_per_million_tokens.get(row['model'], {\"input\": 0, \"output\": 0})\n",
    "        return (row['prompt_tokens'] / 1_000_000) * prices['input'] + (row['completion_tokens'] / 1_000_000) * prices['output']\n",
    "\n",
    "    df_metrics['cost_usd'] = df_metrics.apply(calculate_cost, axis=1)\n",
    "    \n",
    "    display(Markdown(\"### Per-Step Performance and Cost Analysis\"))\n",
    "    display(df_metrics)\n",
    "else:\n",
    "    logging.info(\"No metrics were logged.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "summary-dataframe-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Final Run Summary"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Total Latency (s)</th>\n",
       "      <td>165.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Cost (USD)</th>\n",
       "      <td>$0.003793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Tokens</th>\n",
       "      <td>12,376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>--- Quality Scores (AI) ---</th>\n",
       "      <td>---</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Field Accuracy Score</th>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inference Quality Score</th>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Completeness Score</th>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>**Overall Confidence Score**</th>\n",
       "      <td>**0.85**</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fields for Human Review</th>\n",
       "      <td>mailing_address_if_different_than_risk_address...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          Value\n",
       "Metric                                                                         \n",
       "Total Latency (s)                                                        165.94\n",
       "Total Cost (USD)                                                      $0.003793\n",
       "Total Tokens                                                             12,376\n",
       "--- Quality Scores (AI) ---                                                 ---\n",
       "Field Accuracy Score                                                       0.90\n",
       "Inference Quality Score                                                    0.60\n",
       "Completeness Score                                                         1.00\n",
       "**Overall Confidence Score**                                           **0.85**\n",
       "Fields for Human Review       mailing_address_if_different_than_risk_address..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 5.3 Final Run Summary\n",
    "if metrics_log and quality_assessment:\n",
    "    def get_score(assessment, key):\n",
    "        if isinstance(assessment, dict) and key in assessment and isinstance(assessment[key], dict):\n",
    "            return assessment[key].get('score', 0.0)\n",
    "        return 0.0\n",
    "\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Total Latency (s)',\n",
    "            'Total Cost (USD)',\n",
    "            'Total Tokens',\n",
    "            '--- Quality Scores (AI) ---',\n",
    "            'Field Accuracy Score',\n",
    "            'Inference Quality Score',\n",
    "            'Completeness Score',\n",
    "            '**Overall Confidence Score**',\n",
    "            'Fields for Human Review'\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{df_metrics['latency_s'].sum():.2f}\",\n",
    "            f\"${df_metrics['cost_usd'].sum():.6f}\",\n",
    "            f\"{df_metrics['total_tokens'].sum():,}\",\n",
    "            '---',\n",
    "            f\"{get_score(quality_assessment, 'field_accuracy_score'):.2f}\",\n",
    "            f\"{get_score(quality_assessment, 'inference_quality_score'):.2f}\",\n",
    "            f\"{get_score(quality_assessment, 'completeness_score'):.2f}\",\n",
    "            f\"**{get_score(quality_assessment, 'overall_confidence_score'):.2f}**\",\n",
    "            ', '.join(quality_assessment.get('fields_requiring_human_review', [])) or 'None'\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data).set_index('Metric')\n",
    "    \n",
    "    display(Markdown(\"### Final Run Summary\"))\n",
    "    display(df_summary)\n",
    "else:\n",
    "    logging.info(\"Cannot generate summary as no metrics were logged or quality assessment failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-cell",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook has demonstrated a complete, end-to-end two-stage pipeline for intelligent form processing. By separating the tasks of literal OCR (Stage 1) and logical refinement (Stage 2), we create a system that is both highly accurate and cost-effective.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1.  **The Two-Stage Architecture Excels:** Using a powerful vision model for initial extraction and a cheaper reasoning model for refinement is a highly effective pattern for complex document processing tasks.\n",
    "2.  **Tools are Essential for Reasoning:** The refinement stage's ability to fill in missing information is entirely dependent on its access to tools like web search, demonstrating that reasoning is most powerful when grounded in external data.\n",
    "3.  **Automated Evaluation Provides Actionable Insights:** By adding a final evaluation agent, we transform the pipeline from a black box into a transparent system that quantifies its own performance and provides clear signals for when human intervention is necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-model-selection (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
