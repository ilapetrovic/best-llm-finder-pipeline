{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Building an Agentic RAG System with Open-Source LLMs\n",
    "\n",
    "This notebook demonstrates how to build an advanced Retrieval-Augmented Generation (RAG) system to answer complex questions from a long document. Instead of relying on a traditional vector database, we will employ a multi-agent, hierarchical approach where LLM-powered agents navigate the document's structure dynamically.\n",
    "\n",
    "This method is particularly useful for dense, lengthy texts like legal manuals, research papers, or financial reports, where context is spread across many pages. Our system will mimic how a human researcher would work: first skimming for relevant sections, then drilling down into specific paragraphs, and finally synthesizing an answer based only on the retrieved information.\n",
    "\n",
    "**Key Features of this Approach:**\n",
    "\n",
    "1.  **Zero-Ingestion:** The system can work with new documents instantly without any pre-processing or embedding steps.\n",
    "2.  **Dynamic Retrieval:** The LLM itself decides which parts of the document are relevant, allowing it to handle paraphrased or conceptual questions more effectively.\n",
    "3.  **Traceability:** The system provides precise, paragraph-level citations for every part of its answer, ensuring verifiability.\n",
    "4.  **Customizable:** The entire workflow is built to be compatible with open-source LLMs accessible through OpenAI-compatible APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-config",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, we'll install the necessary libraries and set up our environment. This includes libraries for handling PDFs, interacting with the LLM API, and basic data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU openai requests pypdf nltk transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-config",
   "metadata": {},
   "source": [
    "### 1.1. Import Libraries and Configure LLM Client\n",
    "\n",
    "Next, we'll import all the required modules. We will also configure our LLM client here. This notebook is designed to work with any LLM that provides an OpenAI-compatible API endpoint.\n",
    "\n",
    "**Important:** You must provide your own `API_KEY` and `BASE_URL` in the cell below. We also define different model names for each task (routing, synthesis, verification) to allow for using specialized or cost-effective models at different stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports-and-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Any\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# --- LLM Configuration ---\n",
    "# Replace with your API key and the base URL of your LLM provider\n",
    "API_KEY = \"API_KEY\" # Replace with your actual key\n",
    "BASE_URL = \"https://api.studio.nebius.com/v1/\"\n",
    "\n",
    "# Define the models to be used for different tasks\n",
    "ROUTER_MODEL = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "SYNTHESIS_MODEL = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "EVALUATION_MODEL = \"deepseek-ai/DeepSeek-V3\" # Use a strong model for evaluation tasks\n",
    "\n",
    "# Initialize the OpenAI client to connect to the custom endpoint\n",
    "client = OpenAI(\n",
    "    api_key=API_KEY,\n",
    "    base_url=BASE_URL,\n",
    ")\n",
    "\n",
    "# --- Global Variables ---\n",
    "# We will store performance metrics here throughout the run\n",
    "metrics_log = []\n",
    "\n",
    "# Download necessary NLTK data for sentence tokenization\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-markdown",
   "metadata": {},
   "source": [
    "### 1.2. Initialize Tokenizer for Estimation\n",
    "\n",
    "The primary purpose of a local tokenizer in this pipeline is to **estimate token counts**. This is crucial for two reasons: 1) to avoid sending requests that exceed the model's context window limit, which would cause an API error, and 2) to estimate the cost of an API call before making it.\n",
    "\n",
    "Since the Router Agent receives the largest prompts (containing many document chunks), its context window is the most likely to be exceeded. Therefore, our local tokenizer should be the best possible proxy for the tokenizer used by the `ROUTER_MODEL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "init-tokenizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer for 'meta-llama/Meta-Llama-3.1-8B-Instruct'...\n"
     ]
    }
   ],
   "source": [
    "# Use the tokenizer from our designated ROUTER_MODEL for consistent token counting.\n",
    "print(f\"Initializing tokenizer for '{ROUTER_MODEL}'...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ROUTER_MODEL)\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Estimates the number of tokens in a string using the reference tokenizer.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc-loading-prep",
   "metadata": {},
   "source": [
    "## 2. Document Loading and Preparation\n",
    "\n",
    "Our process begins by loading the source document. For this example, we'll use the *Trademark Trial and Appeal Board Manual of Procedure (TBMP)*, a lengthy legal document that serves as a great test case. We'll download the PDF, extract its text content, and analyze its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "load-doc-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_from_url(url: str, max_pages: int = 920) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a PDF from a URL, extracts text from its pages, and returns it as a single string.\n",
    "    \"\"\"\n",
    "    print(f\"Downloading document from {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure the download was successful\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    pdf_file = BytesIO(response.content)\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    \n",
    "    num_pages_to_process = min(max_pages, len(pdf_reader.pages))\n",
    "    print(f\"Extracting text from {num_pages_to_process} pages...\")\n",
    "    \n",
    "    full_text = \"\"\n",
    "    # Use tqdm for a progress bar during page extraction\n",
    "    for page in tqdm(pdf_reader.pages[:num_pages_to_process], desc=\"Extracting pages\"):\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text += page_text + \"\\n\"\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "run-doc-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading document from https://www.uspto.gov/sites/default/files/documents/tbmp-Master-June2024.pdf...\n",
      "Extracting text from 920 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23ab05ae0bb744139d3aeac58f539774",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting pages:   0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945084 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document loaded successfully.\n",
      "- Total Characters: 3,459,491\n",
      "- Estimated Tokens: 945,084\n",
      "\n",
      "--- Document Preview (first 500 characters) ---\n",
      "TRADEMARK TRIAL AND\n",
      "APPEAL BOARD MANUAL\n",
      "OF PROCEDURE (TBMP)\n",
      " June 2024\n",
      "June   2024\n",
      "United States Patent and Trademark Office\n",
      "PREFACE TO THE JUNE 2024 REVISION\n",
      "The June 2024 revision of the Trademark Trial and Appeal Board Manual of Procedure is an update of the\n",
      "June 2023 edition. This update is moderate in nature and incorporates relevant case law issued between March\n",
      "3, 2023 and March 1, 2024.\n",
      "The title of the manual is abbreviated as “TBMP.” A citation to a section of the manual may be written\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# URL for the TBMP manual\n",
    "tbmp_url = \"https://www.uspto.gov/sites/default/files/documents/tbmp-Master-June2024.pdf\"\n",
    "document_text = load_pdf_from_url(tbmp_url)\n",
    "\n",
    "# Display document statistics\n",
    "char_count = len(document_text)\n",
    "token_count = count_tokens(document_text)\n",
    "print(f\"\\nDocument loaded successfully.\")\n",
    "print(f\"- Total Characters: {char_count:,}\")\n",
    "print(f\"- Estimated Tokens: {token_count:,}\")\n",
    "\n",
    "print(\"\\n--- Document Preview (first 500 characters) ---\")\n",
    "print(document_text[:500])\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical-chunking",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Chunking\n",
    "\n",
    "The document is far too large to fit into a single model context. Instead of creating hundreds of small, independent chunks for a vector database, we will create a small number of large, high-level chunks (e.g., 20). This forms the top level of our document hierarchy.\n",
    "\n",
    "Our chunking function is designed to be \"sentence-aware,\" meaning it tries to avoid splitting sentences in the middle, which helps preserve the semantic integrity of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "chunking-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text: str, num_chunks: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Splits a long text into a specified number of chunks, respecting sentence boundaries.\n",
    "    \"\"\"\n",
    "    # First, split the entire text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    # Calculate how many sentences should go into each chunk on average\n",
    "    sentences_per_chunk = (len(sentences) + num_chunks - 1) // num_chunks\n",
    "\n",
    "    chunks = []\n",
    "    desc = \"Creating chunks\" if len(sentences) > 500 else None # Only show progress bar if it's a long process\n",
    "    for i in tqdm(range(0, len(sentences), sentences_per_chunk), desc=desc):\n",
    "        chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "        chunk_text = \" \".join(chunk_sentences)\n",
    "        chunks.append({\n",
    "            \"id\": len(chunks),  # Assign a simple integer ID\n",
    "            \"text\": chunk_text\n",
    "        })\n",
    "    \n",
    "    print(f\"Document split into {len(chunks)} chunks.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "run-chunking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac29ae813028453eb8613242facb9ace",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating chunks:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 20 chunks.\n",
      "- Chunk 0: 42,822 tokens\n",
      "- Chunk 1: 42,367 tokens\n",
      "- Chunk 2: 42,516 tokens\n"
     ]
    }
   ],
   "source": [
    "document_chunks = split_text_into_chunks(document_text, num_chunks=20)\n",
    "\n",
    "# Display stats for the first few chunks\n",
    "for chunk in document_chunks[:3]:\n",
    "    chunk_token_count = count_tokens(chunk['text'])\n",
    "    print(f\"- Chunk {chunk['id']}: {chunk_token_count:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-nav",
   "metadata": {},
   "source": [
    "## 4. The Agentic Navigation Workflow\n",
    "\n",
    "This is the core of our system. We will create a recursive process that navigates the document hierarchy to find the most relevant paragraphs for a given question. The workflow consists of two main components:\n",
    "\n",
    "1.  **Router Agent:** An LLM that examines a set of chunks and selects which ones are relevant. We implement this as a two-step process: first, the agent writes its reasoning to a \"scratchpad,\" and second, it makes its final selection. This separation improves the quality of its decisions.\n",
    "2.  **Recursive Navigator:** A loop that repeatedly calls the Router Agent. It starts with top-level chunks. When the agent selects chunks, the navigator splits them into smaller sub-chunks and presents those to the agent in the next step. This continues until a maximum depth is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json-parser-helper",
   "metadata": {},
   "source": [
    "### 4.1. Helper Function for Robust JSON Parsing\n",
    "\n",
    "Since we will be asking the LLM to return JSON formatted text, we need a reliable way to parse it. Models sometimes wrap their JSON output in markdown code blocks (e.g., ` ```json ... ``` `) or add explanatory text. This helper function is designed to find and parse the JSON block, making our system more resilient to formatting variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "json-parser-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_from_response(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts and parses a JSON object from a string, even if it's embedded in markdown.\n",
    "    \"\"\"\n",
    "    match = re.search(r'```(?:json)?\\s*({.*?})\\s*```', text, re.S)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}')\n",
    "        if start != -1 and end != -1:\n",
    "            json_str = text[start:end+1]\n",
    "        else:\n",
    "            json_str = text\n",
    "    \n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Failed to parse JSON from response. Raw text: '{text}'\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "router-agent-md",
   "metadata": {},
   "source": [
    "### 4.2. The Router Agent (Two-Pass Logic)\n",
    "\n",
    "This function encapsulates the two-pass routing logic. It first calls the LLM to generate reasoning, then incorporates that reasoning into a second call to make the final selection. This emulates a more deliberate thought process and leads to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "router-agent-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_chunks(question: str, chunks: List[Dict[str, Any]], scratchpad: str, depth: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses a two-pass LLM approach to select relevant chunks for a given question.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Routing at Depth {depth}: Evaluating {len(chunks)} chunks ---\")\n",
    "    \n",
    "    chunks_formatted = \"\\n\\n\".join([f\"CHUNK {chunk['id']}:\\n{chunk['text'][:1000]}...\" for chunk in chunks])\n",
    "    reasoning_prompt = f\"\"\"\n",
    "    You are an expert document analyst. Your goal is to find information to answer the user's question:\n",
    "    '{question}'\n",
    "    \n",
    "    Here is your reasoning so far:\n",
    "    {scratchpad}\n",
    "    \n",
    "    Review the following new text chunks. Briefly explain which chunks seem relevant to the question and why. This is your internal monologue.\n",
    "    \n",
    "    TEXT CHUNKS:\n",
    "    {chunks_formatted}\n",
    "    \n",
    "    Your Reasoning:\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    reasoning_response = client.chat.completions.create(model=ROUTER_MODEL, messages=[{\"role\": \"user\", \"content\": reasoning_prompt}], temperature=0.0)\n",
    "    latency_1 = time.time() - start_time\n",
    "    \n",
    "    new_reasoning = reasoning_response.choices[0].message.content\n",
    "    updated_scratchpad = scratchpad + f\"\\n[Depth {depth} Reasoning]: {new_reasoning}\"\n",
    "    print(f\"LLM Reasoning: {new_reasoning}\")\n",
    "    \n",
    "    p_tokens_1, c_tokens_1 = reasoning_response.usage.prompt_tokens, reasoning_response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": f\"route_depth_{depth}_reason\", \"model\": ROUTER_MODEL, \"latency_s\": latency_1, \"prompt_tokens\": p_tokens_1, \"completion_tokens\": c_tokens_1, \"total_tokens\": p_tokens_1 + c_tokens_1})\n",
    "\n",
    "    selection_prompt = f\"\"\"\n",
    "    Based on your reasoning below, select the chunk IDs that are most likely to contain the answer to the question: '{question}'.\n",
    "    \n",
    "    Your Reasoning:\n",
    "    {new_reasoning}\n",
    "    \n",
    "    TEXT CHUNKS:\n",
    "    {chunks_formatted}\n",
    "    \n",
    "    Respond with ONLY a valid JSON object with a single key 'selected_chunk_ids', which is a list of integers. Example: {{\"selected_chunk_ids\": [1, 5, 8]}}\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    selection_response = client.chat.completions.create(model=ROUTER_MODEL, messages=[{\"role\": \"user\", \"content\": selection_prompt}], temperature=0.0)\n",
    "    latency_2 = time.time() - start_time\n",
    "\n",
    "    response_text = selection_response.choices[0].message.content\n",
    "    parsed_output = parse_json_from_response(response_text)\n",
    "    selected_ids = parsed_output.get('selected_chunk_ids', [])\n",
    "    print(f\"Selected chunk IDs: {selected_ids}\")\n",
    "    \n",
    "    p_tokens_2, c_tokens_2 = selection_response.usage.prompt_tokens, selection_response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": f\"route_depth_{depth}_select\", \"model\": ROUTER_MODEL, \"latency_s\": latency_2, \"prompt_tokens\": p_tokens_2, \"completion_tokens\": c_tokens_2, \"total_tokens\": p_tokens_2 + c_tokens_2})\n",
    "\n",
    "    return {\"selected_ids\": selected_ids, \"scratchpad\": updated_scratchpad}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recursive-nav-md",
   "metadata": {},
   "source": [
    "### 4.3. The Recursive Navigator\n",
    "\n",
    "This function orchestrates the entire navigation process. It initializes the loop with the top-level document chunks and calls the router. For each selected chunk, it splits it into smaller sub-chunks and continues the process until the `max_depth` is reached. The path of each chunk (e.g., `3.5.2`) is tracked to provide clear, hierarchical citations in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "recursive-nav-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_document(question: str, initial_chunks: List[Dict[str, Any]], max_depth: int = 2) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Performs a hierarchical navigation of the document to find relevant paragraphs.\n",
    "    \"\"\"\n",
    "    scratchpad = \"\"\n",
    "    current_chunks = initial_chunks\n",
    "    final_paragraphs = []\n",
    "    \n",
    "    chunk_paths = {chunk[\"id\"]: str(chunk[\"id\"]) for chunk in initial_chunks}\n",
    "\n",
    "    for depth in tqdm(range(max_depth), desc=\"Navigating Document\"):\n",
    "        result = route_to_chunks(question, current_chunks, scratchpad, depth)\n",
    "        scratchpad = result[\"scratchpad\"]\n",
    "        selected_ids = result[\"selected_ids\"]\n",
    "        \n",
    "        if not selected_ids:\n",
    "            print(\"\\nNavigation stopped: No relevant chunks selected.\")\n",
    "            final_paragraphs = current_chunks\n",
    "            break\n",
    "\n",
    "        selected_chunks = [c for c in current_chunks if c[\"id\"] in selected_ids]\n",
    "\n",
    "        next_level_chunks = []\n",
    "        next_chunk_id_counter = 0\n",
    "        for chunk in selected_chunks:\n",
    "            parent_path = chunk_paths[chunk[\"id\"]]\n",
    "            sub_chunks = split_text_into_chunks(chunk['text'], num_chunks=10)\n",
    "            \n",
    "            for i, sub_chunk in enumerate(sub_chunks):\n",
    "                new_id = next_chunk_id_counter\n",
    "                sub_chunk['id'] = new_id\n",
    "                chunk_paths[new_id] = f\"{parent_path}.{i}\"\n",
    "                next_level_chunks.append(sub_chunk)\n",
    "                next_chunk_id_counter += 1\n",
    "        \n",
    "        current_chunks = next_level_chunks\n",
    "        final_paragraphs = current_chunks\n",
    "        \n",
    "    print(f\"\\nNavigation finished. Returning {len(final_paragraphs)} retrieved paragraphs.\")\n",
    "    for chunk in final_paragraphs:\n",
    "        if chunk['id'] in chunk_paths:\n",
    "             chunk['display_id'] = chunk_paths[chunk['id']]\n",
    "        \n",
    "    return {\"paragraphs\": final_paragraphs, \"scratchpad\": scratchpad}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-nav-md",
   "metadata": {},
   "source": [
    "### 4.4. Run the Full Navigation Process\n",
    "\n",
    "Now we'll execute the navigation with a sample question. This will perform multiple LLM calls and drill down into the document to find the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "run-nav-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c0e29a0fdd14182a013ffe45ece07a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Navigating Document:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Routing at Depth 0: Evaluating 20 chunks ---\n",
      "LLM Reasoning: After reviewing the provided text chunks, I've identified several relevant sections that seem to address the requirements for filing a motion to compel discovery, including formatting and signatures. Here's my reasoning:\n",
      "\n",
      "* CHUNK 1 mentions the requirement for representation by an attorney for parties with a domicile outside the United States, but it doesn't directly address the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 2 discusses the process for filing extensions of time to oppose, but it doesn't provide information on the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 3 mentions the process for substituting a real party in interest, but it doesn't address the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 4, 5, and 6 seem to be discussing various aspects of the discovery process, including the requirements for filing answers, interrogatories, and requests for production. However, they don't specifically address the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 7 discusses the process for taking depositions, but it doesn't provide information on the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 8 mentions the requirements for responding to interrogatories, but it doesn't directly address the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 9 discusses the process for filing motions for sanctions, but it mentions that a motion to compel discovery must have already been granted before a motion for sanctions can be filed. This suggests that the requirements for filing a motion to compel discovery may be addressed in a different section.\n",
      "* CHUNK 10 discusses the discoverability of certain information, but it doesn't provide information on the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 11 mentions the requirements for filing counterclaims, but it doesn't directly address the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 12 discusses the process for filing motions to strike, but it doesn't provide information on the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 13 mentions the process for filing motions for additional time for oral argument, but it doesn't address the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 14 discusses the process for filing motions to amend pleadings, but it doesn't provide information on the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 15 mentions the process for stipulating to the admission of evidence, but it doesn't directly address the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 16 discusses the process for serving trial testimony, but it doesn't provide information on the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 17 mentions the admissibility of certain evidence, but it doesn't address the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 18 discusses the process for filing motions to strike, but it doesn't provide information on the requirements for filing a motion to compel discovery.\n",
      "* CHUNK 19 mentions the process for terminating proceedings, but it doesn't address the requirements for filing a motion to compel discovery.\n",
      "\n",
      "However, CHUNK 9 mentions that a motion to compel discovery must have already been granted before a motion for sanctions can be filed. This suggests that the requirements for filing a motion to compel discovery may be addressed in a different section.\n",
      "\n",
      "Based on this analysis, I would recommend searching for text chunks that discuss the requirements for filing motions to compel discovery, such as CHUNK 9, which mentions the process for filing motions for sanctions. I would also consider searching for text chunks that discuss the discovery process in general, such as CHUNK 4, 5, and 6, as they may provide relevant information on the requirements for filing a motion to compel discovery.\n",
      "Selected chunk IDs: [9]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0acd1c5b6f84c70afffc695479880d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating chunks:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 10 chunks.\n",
      "\n",
      "--- Routing at Depth 1: Evaluating 10 chunks ---\n",
      "LLM Reasoning: Based on the new text chunks, I think CHUNK 0, CHUNK 4, and CHUNK 8 seem relevant to the question about the requirements for filing a motion to compel discovery, including formatting and signatures.\n",
      "\n",
      "CHUNK 0 mentions the process for filing motions for sanctions, which is related to the discovery process. Specifically, it states that a motion for sanctions is only appropriate if a motion to compel discovery has already been granted. This suggests that the requirements for filing a motion to compel discovery may be addressed in a different section.\n",
      "\n",
      "CHUNK 4 discusses the process for challenging designations under the Board's standard protective order. It mentions that a motion to challenge a designation should be substantially contemporaneous with the designation, or made as soon as practicable after the basis for the challenge is known. This seems relevant to the discovery process and may provide information on the requirements for filing a motion to compel discovery.\n",
      "\n",
      "CHUNK 8 discusses the process for filing motions for protective orders under Federal Rule of Civil Procedure 26(c). It mentions that the motion must include a certification that the movant has in good faith conferred or attempted to confer with other affected parties in an effort to resolve the dispute without court action. This seems relevant to the discovery process and may provide information on the requirements for filing a motion to compel discovery.\n",
      "\n",
      "I would recommend searching for text chunks that discuss the requirements for filing motions to compel discovery, such as CHUNK 0, CHUNK 4, and CHUNK 8. I would also consider searching for text chunks that discuss the discovery process in general, as they may provide relevant information on the requirements for filing a motion to compel discovery.\n",
      "Selected chunk IDs: [0, 4, 8]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "255d6bdac3ff4023abd8d1e305615990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 10 chunks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22aad2781d2345dda39d6e0be202d285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 10 chunks.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eb7e4b29326416d9d540efefa232390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 10 chunks.\n",
      "\n",
      "Navigation finished. Returning 30 retrieved paragraphs.\n",
      "\n",
      "--- Navigation Complete ---\n",
      "Retrieved 30 paragraphs for synthesis.\n",
      "\n",
      "--- Preview of Retrieved Paragraph 9.0.0 ---\n",
      "See MISCELLANEOUS CHANGES TO TRADEMARK TRIAL AND APPEAL\n",
      "BOARD RULES OF PRACTICE, 81 Fed. Reg. 69950, 69951, 69977 (October 7, 2016). 3. 37 C.F.R. § 2.120(h)(1) and 37 C.F.R. § 2.120(h)(2); MISCELLANEOUS CHANGES TO TRADEMARK\n",
      "TRIAL AND APPEAL BOARD RULES, 72 Fed. Re g. 42242, 42256 (August 1, 2007) (“ A motion for\n",
      "sanctions is only appropriate if a motion to compel these respective disclosures has already been granted.”);\n",
      "Amazon Technologies v. Wax , 93 USPQ2d 1702, 1706 (TTAB 2009) (motion for sa...\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "sample_question = \"What are the requirements for filing a motion to compel discovery, including formatting and signatures?\"\n",
    "\n",
    "metrics_log = [] \n",
    "\n",
    "navigation_result = navigate_document(sample_question, document_chunks, max_depth=2)\n",
    "\n",
    "print(f\"\\n--- Navigation Complete ---\")\n",
    "print(f\"Retrieved {len(navigation_result['paragraphs'])} paragraphs for synthesis.\")\n",
    "\n",
    "if navigation_result['paragraphs']:\n",
    "    first_para = navigation_result['paragraphs'][0]\n",
    "    print(f\"\\n--- Preview of Retrieved Paragraph {first_para.get('display_id', 'N/A')} ---\")\n",
    "    print(first_para['text'][:500] + \"...\")\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answer-synthesis-md",
   "metadata": {},
   "source": [
    "## 5. Answer Synthesis\n",
    "\n",
    "After the navigation phase, we have a curated set of paragraphs that are highly relevant to the question. The next step is to use a **Synthesizer Agent** to generate a comprehensive, human-readable answer based *only* on this retrieved context.\n",
    "\n",
    "We will instruct the model to cite the `display_id` of the paragraphs it uses, ensuring every piece of information in the answer is traceable back to its source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "synthesis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, paragraphs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates a final answer based on the retrieved paragraphs.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Synthesizing Final Answer ---\")\n",
    "    \n",
    "    if not paragraphs:\n",
    "        return {\"answer\": \"I could not find relevant information to answer the question.\", \"citations\": []}\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"PARAGRAPH {p.get('display_id', p['id'])}:\\n{p['text']}\" for p in paragraphs])\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are a legal research assistant. Your task is to answer the user's question based *only* on the provided paragraphs from a legal manual.\n",
    "    - Synthesize the information from the paragraphs into a clear and concise answer.\n",
    "    - For every statement you make, you MUST cite the paragraph ID(s) it is based on in parentheses, like (ID: 1.2.5).\n",
    "    - If the provided paragraphs do not contain enough information, state that clearly.\n",
    "    - Do not use any external knowledge.\n",
    "    - Respond with a JSON object containing 'answer' and 'citations' (a list of all unique IDs you cited).\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    USER QUESTION: \"{question}\"\n",
    "    \n",
    "    SOURCE PARAGRAPHS:\n",
    "    {context}\n",
    "    \n",
    "    Please provide your answer in the required JSON format.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=SYNTHESIS_MODEL, messages=messages, temperature=0.0)\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"synthesis\", \"model\": SYNTHESIS_MODEL, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "    \n",
    "    parsed_output = parse_json_from_response(response_text)\n",
    "    return {\n",
    "        \"answer\": parsed_output.get(\"answer\", \"Failed to generate a valid answer.\"),\n",
    "        \"citations\": sorted(list(set(parsed_output.get(\"citations\", []))))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "run-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Synthesizing Final Answer ---\n",
      "\n",
      "--- GENERATED ANSWER ---\n",
      "The requirements for filing a motion to compel discovery include filing the motion before the deadline for pretrial disclosures for the first testimony period, as originally set or as reset (9.0.1, 9.0.5). The motion must be filed with the Board and must include a certification that the movant has in good faith conferred or attempted to confer with other affected parties in an effort to resolve the dispute without court action (9.0.8.0). The party seeking discovery must demonstrate that the responding party has failed to answer any interrogatory or produce and permit the inspection and copying of any document or thing (9.0.1). The motion to compel must be in writing and must specify the discovery requests that the responding party has failed to answer or produce (9.0.1). The Board may grant the motion to compel if it determines that the responding party has failed to comply with the discovery requests and that the requesting party has made a good faith effort to resolve the dispute without court action (9.0.1, 9.0.5).\n",
      "\n",
      "--- CITATIONS ---\n",
      "['9.0.1', '9.0.5', '9.0.8.0']\n"
     ]
    }
   ],
   "source": [
    "final_answer_result = generate_answer(\n",
    "    sample_question, \n",
    "    navigation_result['paragraphs']\n",
    ")\n",
    "\n",
    "print(\"\\n--- GENERATED ANSWER ---\")\n",
    "print(final_answer_result['answer'])\n",
    "print(\"\\n--- CITATIONS ---\")\n",
    "print(final_answer_result['citations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-eval-md",
   "metadata": {},
   "source": [
    "## 6. Qualitative Evaluation (LLM-as-Judge)\n",
    "\n",
    "To ensure the quality and trustworthiness of our system, we add a final evaluation stage with multiple checks. We use a powerful **Evaluation Agent** to act as a judge on several criteria.\n",
    "\n",
    "1.  **Faithfulness:** Checks if the answer is factually consistent with the cited source paragraphs.\n",
    "2.  **Answer Relevance:** Scores how well the answer addresses the original question.\n",
    "3.  **Retrieval Relevance:** Scores how relevant the retrieved paragraphs were for answering the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faithfulness-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(question: str, answer: str, citations: List[str], paragraphs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses an LLM to verify if the answer is fully supported by the cited paragraphs.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating Answer Faithfulness ---\")\n",
    "    \n",
    "    if not citations or not answer:\n",
    "        return {\"is_faithful\": False, \"explanation\": \"No answer or citations provided.\"}\n",
    "        \n",
    "    cited_paragraphs = [p for p in paragraphs if p.get('display_id') in citations]\n",
    "    if not cited_paragraphs:\n",
    "        return {\"is_faithful\": False, \"explanation\": f\"Cited IDs {citations} not found.\"}\n",
    "        \n",
    "    context = \"\\n\\n\".join([f\"PARAGRAPH {p['display_id']}:\\n{p['text']}\" for p in cited_paragraphs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a meticulous fact-checker. Determine if the 'ANSWER' is fully supported by the 'SOURCE PARAGRAPHS'.\n",
    "    The answer is 'faithful' only if every single piece of information it contains is directly stated or logically derived from the source paragraphs.\n",
    "    \n",
    "    QUESTION: \"{question}\"\n",
    "    ANSWER TO VERIFY: \"{answer}\"\n",
    "    SOURCE PARAGRAPHS:\n",
    "    {context}\n",
    "    \n",
    "    Respond with a JSON object: {{\"is_faithful\": boolean, \"explanation\": \"brief reasoning\"}}.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=EVALUATION_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0.0)\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"eval_faithfulness\", \"model\": EVALUATION_MODEL, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "\n",
    "    return parse_json_from_response(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "relevance-score-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer_relevance(question: str, answer: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scores the relevance of the generated answer to the original question.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating Answer Relevance ---\")\n",
    "    prompt = f\"\"\"\n",
    "    Score how well the 'ANSWER' addresses the 'ORIGINAL QUESTION' on a scale from 0.0 to 1.0.\n",
    "    - A score of 1.0 means the answer completely and directly answers the question.\n",
    "    - A score of 0.0 means the answer is completely irrelevant.\n",
    "    \n",
    "    ORIGINAL QUESTION: \"{question}\"\n",
    "    ANSWER: \"{answer}\"\n",
    "    \n",
    "    Respond with a JSON object: {{\"score\": float, \"justification\": \"brief reasoning\"}}.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=EVALUATION_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0.0)\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"eval_answer_relevance\", \"model\": EVALUATION_MODEL, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "    \n",
    "    parsed = parse_json_from_response(response_text)\n",
    "    return {\"score\": parsed.get(\"score\", 0.0), \"justification\": parsed.get(\"justification\", \"\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "retrieval-score-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_relevance(question: str, paragraphs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scores the relevance of the retrieved documents to the question.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating Retrieval Relevance ---\")\n",
    "    context = \"\\n\\n\".join([f\"PARAGRAPH {p.get('display_id', p['id'])}:\\n{p['text'][:500]}...\" for p in paragraphs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Score how relevant the provided 'RETRIEVED PARAGRAPHS' are for answering the 'ORIGINAL QUESTION' on a scale from 0.0 to 1.0.\n",
    "    - A score of 1.0 means the paragraphs contain all the necessary information.\n",
    "    - A score of 0.0 means the paragraphs are completely irrelevant.\n",
    "    \n",
    "    ORIGINAL QUESTION: \"{question}\"\n",
    "    RETRIEVED PARAGRAPHS:\n",
    "    {context}\n",
    "    \n",
    "    Respond with a JSON object: {{\"score\": float, \"justification\": \"brief reasoning\"}}.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = client.chat.completions.create(model=EVALUATION_MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0.0)\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"eval_retrieval_relevance\", \"model\": EVALUATION_MODEL, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "    \n",
    "    parsed = parse_json_from_response(response_text)\n",
    "    return {\"score\": parsed.get(\"score\", 0.0), \"justification\": parsed.get(\"justification\", \"\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "run-evaluations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Answer Faithfulness ---\n",
      "\n",
      "--- Evaluating Answer Relevance ---\n",
      "\n",
      "--- Evaluating Retrieval Relevance ---\n",
      "\n",
      "--- QUALITATIVE EVALUATION SUMMARY ---\n",
      "Faithfulness Check: FAILED\n",
      "  -> Explanation: The answer includes information about the Board granting the motion to compel if it determines the responding party has failed to comply and the requesting party has made a good faith effort to resolve the dispute, which is not explicitly stated in the source paragraphs. Additionally, the answer mentions the motion must be in writing and specify the discovery requests, which is not directly supported by the provided source paragraphs.\n",
      "Answer Relevance Score: 0.80\n",
      "  -> Justification: The answer provides detailed information on the requirements for filing a motion to compel discovery, including deadlines, certifications, and the necessity of demonstrating non-compliance. However, it does not explicitly mention formatting details or signature requirements, which are part of the original question.\n",
      "Retrieval Relevance Score: 0.20\n",
      "  -> Justification: The retrieved paragraphs primarily discuss general discovery procedures, sanctions, and protective orders, but they do not specifically address the requirements for filing a motion to compel discovery, including formatting and signatures. The information is tangential and lacks the necessary details to answer the original question.\n"
     ]
    }
   ],
   "source": [
    "# Run all qualitative evaluations\n",
    "faithfulness_result = evaluate_faithfulness(\n",
    "    sample_question, \n",
    "    final_answer_result['answer'], \n",
    "    final_answer_result['citations'],\n",
    "    navigation_result['paragraphs']\n",
    ")\n",
    "\n",
    "answer_relevance_result = evaluate_answer_relevance(\n",
    "    sample_question,\n",
    "    final_answer_result['answer']\n",
    ")\n",
    "\n",
    "retrieval_relevance_result = evaluate_retrieval_relevance(\n",
    "    sample_question,\n",
    "    navigation_result['paragraphs']\n",
    ")\n",
    "\n",
    "print(\"\\n--- QUALITATIVE EVALUATION SUMMARY ---\")\n",
    "print(f\"Faithfulness Check: {'PASSED' if faithfulness_result.get('is_faithful') else 'FAILED'}\")\n",
    "print(f\"  -> Explanation: {faithfulness_result.get('explanation')}\")\n",
    "print(f\"Answer Relevance Score: {answer_relevance_result.get('score'):.2f}\")\n",
    "print(f\"  -> Justification: {answer_relevance_result.get('justification')}\")\n",
    "print(f\"Retrieval Relevance Score: {retrieval_relevance_result.get('score'):.2f}\")\n",
    "print(f\"  -> Justification: {retrieval_relevance_result.get('justification')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-analysis-md",
   "metadata": {},
   "source": [
    "## 7. Final Analysis and Summary\n",
    "\n",
    "Finally, we'll consolidate all our metrics—both operational and qualitative—into two clear summaries. The first DataFrame provides a detailed breakdown of each API call, while the second offers a high-level summary of the entire query's performance and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "price-config-md-2",
   "metadata": {},
   "source": [
    "### 7.1. Define Model Pricing\n",
    "\n",
    "Define the cost per million tokens for all models used. You should get this information from your LLM provider. The prices below are placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "price-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prices_per_million_tokens = {\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\": {\n",
    "        \"input\": 0.02,\n",
    "        \"output\": 0.06\n",
    "    },\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": {\n",
    "        \"input\": 0.13,\n",
    "        \"output\": 0.40\n",
    "    },\n",
    "    \"deepseek-ai/DeepSeek-V3\": {\n",
    "        \"input\": 0.50,\n",
    "        \"output\": 1.50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-dataframe-md",
   "metadata": {},
   "source": [
    "### 7.2. Per-Step Operational Metrics\n",
    "\n",
    "This first DataFrame shows the detailed operational cost and latency for every individual LLM call made during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dataframe-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-Step Performance and Cost Analysis ---\n",
      "                       step                                  model  latency_s  prompt_tokens  completion_tokens  total_tokens  cost_usd\n",
      "0      route_depth_0_reason  meta-llama/Meta-Llama-3.1-8B-Instruct  14.847801           6139                734          6873  0.000167\n",
      "1      route_depth_0_select  meta-llama/Meta-Llama-3.1-8B-Instruct   0.879542           6877                 12          6889  0.000138\n",
      "2      route_depth_1_reason  meta-llama/Meta-Llama-3.1-8B-Instruct   5.627172           3706                330          4036  0.000094\n",
      "3      route_depth_1_select  meta-llama/Meta-Llama-3.1-8B-Instruct   0.698597           3299                 18          3317  0.000067\n",
      "4                 synthesis      meta-llama/Llama-3.3-70B-Instruct   9.360613          12301                265         12566  0.001705\n",
      "5         eval_faithfulness                deepseek-ai/DeepSeek-V3   3.267365           1556                 97          1653  0.000924\n",
      "6     eval_answer_relevance                deepseek-ai/DeepSeek-V3   2.444127            340                 69           409  0.000273\n",
      "7  eval_retrieval_relevance                deepseek-ai/DeepSeek-V3   3.081374           4755                 70          4825  0.002482\n"
     ]
    }
   ],
   "source": [
    "if metrics_log:\n",
    "    df_metrics = pd.DataFrame(metrics_log)\n",
    "\n",
    "    def calculate_cost(row):\n",
    "        model_name = row['model']\n",
    "        prices = model_prices_per_million_tokens.get(model_name, {\"input\": 0, \"output\": 0})\n",
    "        input_cost = (row['prompt_tokens'] / 1_000_000) * prices['input']\n",
    "        output_cost = (row['completion_tokens'] / 1_000_000) * prices['output']\n",
    "        return input_cost + output_cost\n",
    "\n",
    "    df_metrics['cost_usd'] = df_metrics.apply(calculate_cost, axis=1)\n",
    "    \n",
    "    print(\"--- Per-Step Performance and Cost Analysis ---\")\n",
    "    print(df_metrics.to_string())\n",
    "else:\n",
    "    print(\"No metrics were logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-dataframe-md",
   "metadata": {},
   "source": [
    "### 7.3. Final Query Summary\n",
    "\n",
    "This final summary DataFrame provides a holistic, single-row view of the entire query, combining operational totals with the crucial qualitative scores to assess overall success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "summary-dataframe-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Query Summary ---\n",
      "                                                                      Result\n",
      "question                   What are the requirements for filing a motion ...\n",
      "total_latency_s                                                     40.20659\n",
      "total_cost_usd                                                      0.005851\n",
      "total_tokens                                                           40568\n",
      "faithfulness_check                                                    FAILED\n",
      "answer_relevance_score                                                   0.8\n",
      "retrieval_relevance_score                                                0.2\n",
      "overall_confidence_score                                                 0.0\n"
     ]
    }
   ],
   "source": [
    "if metrics_log:\n",
    "    # Calculate totals from the detailed metrics log\n",
    "    total_latency = df_metrics['latency_s'].sum()\n",
    "    total_cost = df_metrics['cost_usd'].sum()\n",
    "    total_tokens = df_metrics['total_tokens'].sum()\n",
    "    \n",
    "    # Get qualitative scores\n",
    "    faithfulness_score = 1.0 if faithfulness_result.get('is_faithful') else 0.0\n",
    "    answer_relevance_score = answer_relevance_result.get('score', 0.0)\n",
    "    retrieval_relevance_score = retrieval_relevance_result.get('score', 0.0)\n",
    "    \n",
    "    # Calculate a simple overall confidence score\n",
    "    overall_confidence = faithfulness_score * answer_relevance_score * retrieval_relevance_score\n",
    "\n",
    "    # Create summary dictionary\n",
    "    summary_data = {\n",
    "        'question': [sample_question],\n",
    "        'total_latency_s': [total_latency],\n",
    "        'total_cost_usd': [total_cost],\n",
    "        'total_tokens': [total_tokens],\n",
    "        'faithfulness_check': ['PASSED' if faithfulness_score == 1.0 else 'FAILED'],\n",
    "        'answer_relevance_score': [answer_relevance_score],\n",
    "        'retrieval_relevance_score': [retrieval_relevance_score],\n",
    "        'overall_confidence_score': [overall_confidence]\n",
    "    }\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"--- Final Query Summary ---\")\n",
    "    # Transpose for better readability of a single-row summary\n",
    "    print(df_summary.T.rename(columns={0: 'Result'}))\n",
    "else:\n",
    "    print(\"Cannot generate summary as no metrics were logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-cell",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook has demonstrated an end-to-end agentic RAG workflow using customizable, open-source LLMs. By employing a hierarchical navigation strategy and a multi-step qualitative evaluation, we can tackle complex questions in long documents with high precision and full traceability, all without the overhead of a traditional vector database.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1.  **Agents Offer Control:** Breaking the problem into specialized agents (Router, Synthesizer, Evaluator) provides greater control and allows for the use of different models optimized for each task.\n",
    "2.  **Hierarchical Navigation is Powerful:** This approach effectively narrows down a vast search space, mimicking human-like research patterns.\n",
    "3.  **LLM-based Evaluation is Crucial:** Moving beyond simple operational metrics to assess faithfulness and relevance is key to building trustworthy and reliable AI systems.\n",
    "4.  **Comprehensive Analysis:** Consolidating operational and qualitative metrics into a final summary provides a clear, holistic view of system performance for each query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-model-selection (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
