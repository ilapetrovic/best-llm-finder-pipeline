{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-header",
   "metadata": {},
   "source": [
    "# Building an Agentic RAG System with Open-Source LLMs\n",
    "\n",
    "This notebook demonstrates how to build an advanced Retrieval-Augmented Generation (RAG) system to answer complex questions from a long document. Instead of relying on a traditional vector database, we will employ a multi-agent, hierarchical approach where LLM-powered agents navigate the document's structure dynamically.\n",
    "\n",
    "This method is particularly useful for dense, lengthy texts like legal manuals, research papers, or financial reports, where context is spread across many pages. Our system will mimic how a human researcher would work: first skimming for relevant sections, then drilling down into specific paragraphs, and finally synthesizing an answer based only on the retrieved information.\n",
    "\n",
    "**Key Features of this Approach:**\n",
    "\n",
    "1.  **Zero-Ingestion:** The system can work with new documents instantly without any pre-processing or embedding steps.\n",
    "2.  **Dynamic Retrieval:** The LLM itself decides which parts of the document are relevant, allowing it to handle paraphrased or conceptual questions more effectively.\n",
    "3.  **Traceability:** The system provides precise, paragraph-level citations for every part of its answer, ensuring verifiability.\n",
    "4.  **Customizable:** The entire workflow is built to be compatible with open-source LLMs accessible through OpenAI-compatible APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-config",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "First, we'll install the necessary libraries and set up our environment. This includes libraries for handling PDFs, interacting with the LLM API, and basic data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -qU openai requests pypdf nltk transformers pandas tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "import-config",
   "metadata": {},
   "source": [
    "### 1.1. Import Libraries and Configure LLM Client\n",
    "\n",
    "Next, we'll import all the required modules. We will also configure our LLM client here. This notebook is designed to work with any LLM that provides an OpenAI-compatible API endpoint.\n",
    "\n",
    "**Important:** You must provide your own `API_KEY` and `BASE_URL` in the cell below. We also define different model names for each task (routing, synthesis, verification) to allow for using specialized or cost-effective models at different stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "imports-and-client",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/u/ilacp2/.conda/envs/llm/lib/python3.10/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "307d2612132b4dcaaff5e87e88d6eaa1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pypdf import PdfReader\n",
    "from transformers import AutoTokenizer\n",
    "from typing import List, Dict, Any\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# Choose a local model you can actually load (8B is OK for A40 with 46GB)\n",
    "ROUTER_MODEL = \"meta-llama/Llama-3.1-8B\"\n",
    "SYNTHESIS_MODEL = \"meta-llama/Llama-3.1-8B\"  # keep same to simplify\n",
    "EVALUATION_MODEL = \"meta-llama/Llama-3.1-8B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ROUTER_MODEL)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    ROUTER_MODEL,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# --- Global Variables ---\n",
    "# We will store performance metrics here throughout the run\n",
    "metrics_log = []\n",
    "\n",
    "# Download necessary NLTK data for sentence tokenization\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f0ae7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_llm_generate(prompt, model, tokenizer, max_new_tokens=512):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(**inputs, max_new_tokens=512, do_sample=False)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tokenizer-markdown",
   "metadata": {},
   "source": [
    "### 1.2. Initialize Tokenizer for Estimation\n",
    "\n",
    "The primary purpose of a local tokenizer in this pipeline is to **estimate token counts**. This is crucial for two reasons: 1) to avoid sending requests that exceed the model's context window limit, which would cause an API error, and 2) to estimate the cost of an API call before making it.\n",
    "\n",
    "Since the Router Agent receives the largest prompts (containing many document chunks), its context window is the most likely to be exceeded. Therefore, our local tokenizer should be the best possible proxy for the tokenizer used by the `ROUTER_MODEL`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "init-tokenizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizer for 'meta-llama/Llama-3.1-8B'...\n"
     ]
    }
   ],
   "source": [
    "# Use the tokenizer from our designated ROUTER_MODEL for consistent token counting.\n",
    "print(f\"Initializing tokenizer for '{ROUTER_MODEL}'...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(ROUTER_MODEL)\n",
    "\n",
    "def count_tokens(text: str) -> int:\n",
    "    \"\"\"Estimates the number of tokens in a string using the reference tokenizer.\"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return 0\n",
    "    return len(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "doc-loading-prep",
   "metadata": {},
   "source": [
    "## 2. Document Loading and Preparation\n",
    "\n",
    "Our process begins by loading the source document. For this example, we'll use the *Trademark Trial and Appeal Board Manual of Procedure (TBMP)*, a lengthy legal document that serves as a great test case. We'll download the PDF, extract its text content, and analyze its size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "load-doc-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pdf_from_url(url: str, max_pages: int = 920) -> str:\n",
    "    \"\"\"\n",
    "    Downloads a PDF from a URL, extracts text from its pages, and returns it as a single string.\n",
    "    \"\"\"\n",
    "    print(f\"Downloading document from {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Ensure the download was successful\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    pdf_file = BytesIO(response.content)\n",
    "    pdf_reader = PdfReader(pdf_file)\n",
    "    \n",
    "    num_pages_to_process = min(max_pages, len(pdf_reader.pages))\n",
    "    print(f\"Extracting text from {num_pages_to_process} pages...\")\n",
    "    \n",
    "    full_text = \"\"\n",
    "    # Use tqdm for a progress bar during page extraction\n",
    "    for page in tqdm(pdf_reader.pages[:num_pages_to_process], desc=\"Extracting pages\"):\n",
    "        page_text = page.extract_text()\n",
    "        if page_text:\n",
    "            full_text += page_text + \"\\n\"\n",
    "    \n",
    "    return full_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "run-doc-loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading document from https://www.uspto.gov/sites/default/files/documents/tbmp-Master-June2024.pdf...\n",
      "Extracting text from 920 pages...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0b3fab19f140ee933e233e32a9fae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting pages:   0%|          | 0/920 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (945084 > 131072). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Document loaded successfully.\n",
      "- Total Characters: 3,459,491\n",
      "- Estimated Tokens: 945,084\n",
      "\n",
      "--- Document Preview (first 500 characters) ---\n",
      "TRADEMARK TRIAL AND\n",
      "APPEAL BOARD MANUAL\n",
      "OF PROCEDURE (TBMP)\n",
      " June 2024\n",
      "June   2024\n",
      "United States Patent and Trademark Office\n",
      "PREFACE TO THE JUNE 2024 REVISION\n",
      "The June 2024 revision of the Trademark Trial and Appeal Board Manual of Procedure is an update of the\n",
      "June 2023 edition. This update is moderate in nature and incorporates relevant case law issued between March\n",
      "3, 2023 and March 1, 2024.\n",
      "The title of the manual is abbreviated as “TBMP.” A citation to a section of the manual may be written\n",
      "---------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# URL for the TBMP manual\n",
    "tbmp_url = \"https://www.uspto.gov/sites/default/files/documents/tbmp-Master-June2024.pdf\"\n",
    "document_text = load_pdf_from_url(tbmp_url)\n",
    "\n",
    "# Display document statistics\n",
    "char_count = len(document_text)\n",
    "token_count = count_tokens(document_text)\n",
    "print(f\"\\nDocument loaded successfully.\")\n",
    "print(f\"- Total Characters: {char_count:,}\")\n",
    "print(f\"- Estimated Tokens: {token_count:,}\")\n",
    "\n",
    "print(\"\\n--- Document Preview (first 500 characters) ---\")\n",
    "print(document_text[:500])\n",
    "print(\"---------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hierarchical-chunking",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Chunking\n",
    "\n",
    "The document is far too large to fit into a single model context. Instead of creating hundreds of small, independent chunks for a vector database, we will create a small number of large, high-level chunks (e.g., 20). This forms the top level of our document hierarchy.\n",
    "\n",
    "Our chunking function is designed to be \"sentence-aware,\" meaning it tries to avoid splitting sentences in the middle, which helps preserve the semantic integrity of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "chunking-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_into_chunks(text: str, num_chunks: int = 20) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Splits a long text into a specified number of chunks, respecting sentence boundaries.\n",
    "    \"\"\"\n",
    "    # First, split the entire text into individual sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    if not sentences:\n",
    "        return []\n",
    "\n",
    "    # Calculate how many sentences should go into each chunk on average\n",
    "    sentences_per_chunk = (len(sentences) + num_chunks - 1) // num_chunks\n",
    "\n",
    "    chunks = []\n",
    "    desc = \"Creating chunks\" if len(sentences) > 500 else None # Only show progress bar if it's a long process\n",
    "    for i in tqdm(range(0, len(sentences), sentences_per_chunk), desc=desc):\n",
    "        chunk_sentences = sentences[i:i + sentences_per_chunk]\n",
    "        chunk_text = \" \".join(chunk_sentences)\n",
    "        chunks.append({\n",
    "            \"id\": len(chunks),  # Assign a simple integer ID\n",
    "            \"text\": chunk_text\n",
    "        })\n",
    "    \n",
    "    print(f\"Document split into {len(chunks)} chunks.\")\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "run-chunking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "167d963fcd784735b58f2ef8d69dafab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating chunks:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document split into 20 chunks.\n",
      "- Chunk 0: 42,822 tokens\n",
      "- Chunk 1: 42,367 tokens\n",
      "- Chunk 2: 42,516 tokens\n"
     ]
    }
   ],
   "source": [
    "document_chunks = split_text_into_chunks(document_text, num_chunks=20)\n",
    "\n",
    "# Display stats for the first few chunks\n",
    "for chunk in document_chunks[:3]:\n",
    "    chunk_token_count = count_tokens(chunk['text'])\n",
    "    print(f\"- Chunk {chunk['id']}: {chunk_token_count:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agentic-nav",
   "metadata": {},
   "source": [
    "## 4. The Agentic Navigation Workflow\n",
    "\n",
    "This is the core of our system. We will create a recursive process that navigates the document hierarchy to find the most relevant paragraphs for a given question. The workflow consists of two main components:\n",
    "\n",
    "1.  **Router Agent:** An LLM that examines a set of chunks and selects which ones are relevant. We implement this as a two-step process: first, the agent writes its reasoning to a \"scratchpad,\" and second, it makes its final selection. This separation improves the quality of its decisions.\n",
    "2.  **Recursive Navigator:** A loop that repeatedly calls the Router Agent. It starts with top-level chunks. When the agent selects chunks, the navigator splits them into smaller sub-chunks and presents those to the agent in the next step. This continues until a maximum depth is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "json-parser-helper",
   "metadata": {},
   "source": [
    "### 4.1. Helper Function for Robust JSON Parsing\n",
    "\n",
    "Since we will be asking the LLM to return JSON formatted text, we need a reliable way to parse it. Models sometimes wrap their JSON output in markdown code blocks (e.g., ` ```json ... ``` `) or add explanatory text. This helper function is designed to find and parse the JSON block, making our system more resilient to formatting variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "json-parser-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_from_response(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts and parses a JSON object from a string, even if it's embedded in markdown.\n",
    "    \"\"\"\n",
    "    match = re.search(r'```(?:json)?\\s*({.*?})\\s*```', text, re.S)\n",
    "    if match:\n",
    "        json_str = match.group(1)\n",
    "    else:\n",
    "        start = text.find('{')\n",
    "        end = text.rfind('}')\n",
    "        if start != -1 and end != -1:\n",
    "            json_str = text[start:end+1]\n",
    "        else:\n",
    "            json_str = text\n",
    "    \n",
    "    try:\n",
    "        return json.loads(json_str)\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Warning: Failed to parse JSON from response. Raw text: '{text}'\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "router-agent-md",
   "metadata": {},
   "source": [
    "### 4.2. The Router Agent (Two-Pass Logic)\n",
    "\n",
    "This function encapsulates the two-pass routing logic. It first calls the LLM to generate reasoning, then incorporates that reasoning into a second call to make the final selection. This emulates a more deliberate thought process and leads to better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "router-agent-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_to_chunks(question: str, chunks: List[Dict[str, Any]], scratchpad: str, depth: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses a two-pass LLM approach to select relevant chunks for a given question.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Routing at Depth {depth}: Evaluating {len(chunks)} chunks ---\")\n",
    "    \n",
    "    chunks_formatted = \"\\n\\n\".join([f\"CHUNK {chunk['id']}:\\n{chunk['text'][:1000]}...\" for chunk in chunks])\n",
    "    reasoning_prompt = f\"\"\"\n",
    "    You are an expert document analyst. Your goal is to find information to answer the user's question:\n",
    "    '{question}'\n",
    "    \n",
    "    Here is your reasoning so far:\n",
    "    {scratchpad}\n",
    "    \n",
    "    Review the following new text chunks. Briefly explain which chunks seem relevant to the question and why. This is your internal monologue.\n",
    "    \n",
    "    TEXT CHUNKS:\n",
    "    {chunks_formatted}\n",
    "    \n",
    "    Your Reasoning:\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    reasoning_response = local_llm_generate(reasoning_prompt, model, tokenizer)\n",
    "    latency_1 = time.time() - start_time\n",
    "    \n",
    "    new_reasoning = reasoning_response.choices[0].message.content\n",
    "    updated_scratchpad = scratchpad + f\"\\n[Depth {depth} Reasoning]: {new_reasoning}\"\n",
    "    print(f\"LLM Reasoning: {new_reasoning}\")\n",
    "    \n",
    "    p_tokens_1, c_tokens_1 = reasoning_response.usage.prompt_tokens, reasoning_response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": f\"route_depth_{depth}_reason\", \"model\": ROUTER_MODEL, \"latency_s\": latency_1, \"prompt_tokens\": p_tokens_1, \"completion_tokens\": c_tokens_1, \"total_tokens\": p_tokens_1 + c_tokens_1})\n",
    "\n",
    "    selection_prompt = f\"\"\"\n",
    "    Based on your reasoning below, select the chunk IDs that are most likely to contain the answer to the question: '{question}'.\n",
    "    \n",
    "    Your Reasoning:\n",
    "    {new_reasoning}\n",
    "    \n",
    "    TEXT CHUNKS:\n",
    "    {chunks_formatted}\n",
    "    \n",
    "    Respond with ONLY a valid JSON object with a single key 'selected_chunk_ids', which is a list of integers. Example: {{\"selected_chunk_ids\": [1, 5, 8]}}\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    selection_response = local_llm_generate(reasoning_prompt, model, tokenizer)\n",
    "\n",
    "    latency_2 = time.time() - start_time\n",
    "\n",
    "    response_text = selection_response.choices[0].message.content\n",
    "    parsed_output = parse_json_from_response(response_text)\n",
    "    selected_ids = parsed_output.get('selected_chunk_ids', [])\n",
    "    print(f\"Selected chunk IDs: {selected_ids}\")\n",
    "    \n",
    "    p_tokens_2, c_tokens_2 = selection_response.usage.prompt_tokens, selection_response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": f\"route_depth_{depth}_select\", \"model\": ROUTER_MODEL, \"latency_s\": latency_2, \"prompt_tokens\": p_tokens_2, \"completion_tokens\": c_tokens_2, \"total_tokens\": p_tokens_2 + c_tokens_2})\n",
    "\n",
    "    return {\"selected_ids\": selected_ids, \"scratchpad\": updated_scratchpad}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recursive-nav-md",
   "metadata": {},
   "source": [
    "### 4.3. The Recursive Navigator\n",
    "\n",
    "This function orchestrates the entire navigation process. It initializes the loop with the top-level document chunks and calls the router. For each selected chunk, it splits it into smaller sub-chunks and continues the process until the `max_depth` is reached. The path of each chunk (e.g., `3.5.2`) is tracked to provide clear, hierarchical citations in the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "recursive-nav-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_document(question: str, initial_chunks: List[Dict[str, Any]], max_depth: int = 2) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Performs a hierarchical navigation of the document to find relevant paragraphs.\n",
    "    \"\"\"\n",
    "    scratchpad = \"\"\n",
    "    current_chunks = initial_chunks\n",
    "    final_paragraphs = []\n",
    "    \n",
    "    chunk_paths = {chunk[\"id\"]: str(chunk[\"id\"]) for chunk in initial_chunks}\n",
    "\n",
    "    for depth in tqdm(range(max_depth), desc=\"Navigating Document\"):\n",
    "        result = route_to_chunks(question, current_chunks, scratchpad, depth)\n",
    "        scratchpad = result[\"scratchpad\"]\n",
    "        selected_ids = result[\"selected_ids\"]\n",
    "        \n",
    "        if not selected_ids:\n",
    "            print(\"\\nNavigation stopped: No relevant chunks selected.\")\n",
    "            final_paragraphs = current_chunks\n",
    "            break\n",
    "\n",
    "        selected_chunks = [c for c in current_chunks if c[\"id\"] in selected_ids]\n",
    "\n",
    "        next_level_chunks = []\n",
    "        next_chunk_id_counter = 0\n",
    "        for chunk in selected_chunks:\n",
    "            parent_path = chunk_paths[chunk[\"id\"]]\n",
    "            sub_chunks = split_text_into_chunks(chunk['text'], num_chunks=10)\n",
    "            \n",
    "            for i, sub_chunk in enumerate(sub_chunks):\n",
    "                new_id = next_chunk_id_counter\n",
    "                sub_chunk['id'] = new_id\n",
    "                chunk_paths[new_id] = f\"{parent_path}.{i}\"\n",
    "                next_level_chunks.append(sub_chunk)\n",
    "                next_chunk_id_counter += 1\n",
    "        \n",
    "        current_chunks = next_level_chunks\n",
    "        final_paragraphs = current_chunks\n",
    "        \n",
    "    print(f\"\\nNavigation finished. Returning {len(final_paragraphs)} retrieved paragraphs.\")\n",
    "    for chunk in final_paragraphs:\n",
    "        if chunk['id'] in chunk_paths:\n",
    "             chunk['display_id'] = chunk_paths[chunk['id']]\n",
    "        \n",
    "    return {\"paragraphs\": final_paragraphs, \"scratchpad\": scratchpad}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "run-nav-md",
   "metadata": {},
   "source": [
    "### 4.4. Run the Full Navigation Process\n",
    "\n",
    "Now we'll execute the navigation with a sample question. This will perform multiple LLM calls and drill down into the document to find the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "run-nav-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7396d35388964fd293c159411ef751fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Navigating Document:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Routing at Depth 0: Evaluating 20 chunks ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m sample_question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat are the requirements for filing a motion to compel discovery, including formatting and signatures?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m metrics_log \u001b[38;5;241m=\u001b[39m [] \n\u001b[0;32m----> 5\u001b[0m navigation_result \u001b[38;5;241m=\u001b[39m \u001b[43mnavigate_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_question\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocument_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Navigation Complete ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrieved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(navigation_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparagraphs\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m paragraphs for synthesis.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m, in \u001b[0;36mnavigate_document\u001b[0;34m(question, initial_chunks, max_depth)\u001b[0m\n\u001b[1;32m      9\u001b[0m chunk_paths \u001b[38;5;241m=\u001b[39m {chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]: \u001b[38;5;28mstr\u001b[39m(chunk[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m initial_chunks}\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(max_depth), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNavigating Document\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 12\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mroute_to_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscratchpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     scratchpad \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscratchpad\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m     selected_ids \u001b[38;5;241m=\u001b[39m result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mselected_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mroute_to_chunks\u001b[0;34m(question, chunks, scratchpad, depth)\u001b[0m\n\u001b[1;32m      8\u001b[0m reasoning_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124mYou are an expert document analyst. Your goal is to find information to answer the user\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms question:\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124mYour Reasoning:\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     23\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 24\u001b[0m reasoning_response \u001b[38;5;241m=\u001b[39m \u001b[43mlocal_llm_generate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreasoning_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m latency_1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m     27\u001b[0m new_reasoning \u001b[38;5;241m=\u001b[39m reasoning_response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "Cell \u001b[0;32mIn[13], line 3\u001b[0m, in \u001b[0;36mlocal_llm_generate\u001b[0;34m(prompt, model, tokenizer, max_new_tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mlocal_llm_generate\u001b[39m(prompt, model, tokenizer, max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m):\n\u001b[1;32m      2\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m----> 3\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[1;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[0;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2565\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2566\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2570\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2571\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2572\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[1;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2579\u001b[0m ):\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/generation/utils.py:2784\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2781\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 2784\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2785\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2786\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:918\u001b[0m, in \u001b[0;36mcan_return_tuple.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_dict_passed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     return_dict \u001b[38;5;241m=\u001b[39m return_dict_passed\n\u001b[0;32m--> 918\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    920\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_tuple()\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:459\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;129m@can_return_tuple\u001b[39m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@auto_docstring\u001b[39m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Unpack[TransformersKwargs],\n\u001b[1;32m    441\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CausalLMOutputWithPast:\n\u001b[1;32m    442\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;124;03m    Example:\u001b[39;00m\n\u001b[1;32m    444\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[38;5;124;03m    \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;124;03m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 459\u001b[0m     outputs: BaseModelOutputWithPast \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    462\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    464\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    468\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/utils/generic.py:1064\u001b[0m, in \u001b[0;36mcheck_model_inputs.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m                 monkey_patched_layers\u001b[38;5;241m.\u001b[39mappend((module, original_forward))\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1064\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[1;32m   1067\u001b[0m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m     kwargs_without_recordable \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:395\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, cache_position, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    392\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_emb(hidden_states, position_ids)\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers[: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers]:\n\u001b[0;32m--> 395\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(hidden_states)\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m BaseModelOutputWithPast(\n\u001b[1;32m    407\u001b[0m     last_hidden_state\u001b[38;5;241m=\u001b[39mhidden_states,\n\u001b[1;32m    408\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:294\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_values, use_cache, cache_position, position_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 294\u001b[0m hidden_states, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/utils/deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py:264\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    252\u001b[0m attn_output, attn_weights \u001b[38;5;241m=\u001b[39m attention_interface(\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    254\u001b[0m     query_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    261\u001b[0m )\n\u001b[1;32m    263\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m*\u001b[39minput_shape, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m--> 264\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mo_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, attn_weights\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llm/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sample_question = \"What are the requirements for filing a motion to compel discovery, including formatting and signatures?\"\n",
    "\n",
    "metrics_log = [] \n",
    "\n",
    "navigation_result = navigate_document(sample_question, document_chunks, max_depth=2)\n",
    "\n",
    "print(f\"\\n--- Navigation Complete ---\")\n",
    "print(f\"Retrieved {len(navigation_result['paragraphs'])} paragraphs for synthesis.\")\n",
    "\n",
    "if navigation_result['paragraphs']:\n",
    "    first_para = navigation_result['paragraphs'][0]\n",
    "    print(f\"\\n--- Preview of Retrieved Paragraph {first_para.get('display_id', 'N/A')} ---\")\n",
    "    print(first_para['text'][:500] + \"...\")\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answer-synthesis-md",
   "metadata": {},
   "source": [
    "## 5. Answer Synthesis\n",
    "\n",
    "After the navigation phase, we have a curated set of paragraphs that are highly relevant to the question. The next step is to use a **Synthesizer Agent** to generate a comprehensive, human-readable answer based *only* on this retrieved context.\n",
    "\n",
    "We will instruct the model to cite the `display_id` of the paragraphs it uses, ensuring every piece of information in the answer is traceable back to its source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "synthesis-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(question: str, paragraphs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Generates a final answer based on the retrieved paragraphs.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Synthesizing Final Answer ---\")\n",
    "    \n",
    "    if not paragraphs:\n",
    "        return {\"answer\": \"I could not find relevant information to answer the question.\", \"citations\": []}\n",
    "\n",
    "    context = \"\\n\\n\".join([f\"PARAGRAPH {p.get('display_id', p['id'])}:\\n{p['text']}\" for p in paragraphs])\n",
    "    \n",
    "    system_prompt = \"\"\"\n",
    "    You are a legal research assistant. Your task is to answer the user's question based *only* on the provided paragraphs from a legal manual.\n",
    "    - Synthesize the information from the paragraphs into a clear and concise answer.\n",
    "    - For every statement you make, you MUST cite the paragraph ID(s) it is based on in parentheses, like (ID: 1.2.5).\n",
    "    - If the provided paragraphs do not contain enough information, state that clearly.\n",
    "    - Do not use any external knowledge.\n",
    "    - Respond with a JSON object containing 'answer' and 'citations' (a list of all unique IDs you cited).\n",
    "    \"\"\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "    USER QUESTION: \"{question}\"\n",
    "    \n",
    "    SOURCE PARAGRAPHS:\n",
    "    {context}\n",
    "    \n",
    "    Please provide your answer in the required JSON format.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = local_llm_generate(system_prompt, model, tokenizer)\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"synthesis\", \"model\": SYNTHESIS_MODEL, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "    \n",
    "    parsed_output = parse_json_from_response(response_text)\n",
    "    return {\n",
    "        \"answer\": parsed_output.get(\"answer\", \"Failed to generate a valid answer.\"),\n",
    "        \"citations\": sorted(list(set(parsed_output.get(\"citations\", []))))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "run-synthesis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Synthesizing Final Answer ---\n",
      "\n",
      "--- GENERATED ANSWER ---\n",
      "The requirements for filing a motion to compel discovery include filing the motion before the deadline for pretrial disclosures for the first testimony period, as originally set or as reset (9.0.1, 9.0.5). The motion must be filed with the Board and must include a certification that the movant has in good faith conferred or attempted to confer with other affected parties in an effort to resolve the dispute without court action (9.0.8.0). The party seeking discovery must demonstrate that the responding party has failed to answer any interrogatory or produce and permit the inspection and copying of any document or thing (9.0.1). The motion to compel must be in writing and must specify the discovery requests that the responding party has failed to answer or produce (9.0.1). The Board may grant the motion to compel if it determines that the responding party has failed to comply with the discovery requests and that the requesting party has made a good faith effort to resolve the dispute without court action (9.0.1, 9.0.5).\n",
      "\n",
      "--- CITATIONS ---\n",
      "['9.0.1', '9.0.5', '9.0.8.0']\n"
     ]
    }
   ],
   "source": [
    "final_answer_result = generate_answer(\n",
    "    sample_question, \n",
    "    navigation_result['paragraphs']\n",
    ")\n",
    "\n",
    "print(\"\\n--- GENERATED ANSWER ---\")\n",
    "print(final_answer_result['answer'])\n",
    "print(\"\\n--- CITATIONS ---\")\n",
    "print(final_answer_result['citations'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quality-eval-md",
   "metadata": {},
   "source": [
    "## 6. Qualitative Evaluation (LLM-as-Judge)\n",
    "\n",
    "To ensure the quality and trustworthiness of our system, we add a final evaluation stage with multiple checks. We use a powerful **Evaluation Agent** to act as a judge on several criteria.\n",
    "\n",
    "1.  **Faithfulness:** Checks if the answer is factually consistent with the cited source paragraphs.\n",
    "2.  **Answer Relevance:** Scores how well the answer addresses the original question.\n",
    "3.  **Retrieval Relevance:** Scores how relevant the retrieved paragraphs were for answering the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faithfulness-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_faithfulness(question: str, answer: str, citations: List[str], paragraphs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Uses an LLM to verify if the answer is fully supported by the cited paragraphs.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating Answer Faithfulness ---\")\n",
    "    \n",
    "    if not citations or not answer:\n",
    "        return {\"is_faithful\": False, \"explanation\": \"No answer or citations provided.\"}\n",
    "        \n",
    "    cited_paragraphs = [p for p in paragraphs if p.get('display_id') in citations]\n",
    "    if not cited_paragraphs:\n",
    "        return {\"is_faithful\": False, \"explanation\": f\"Cited IDs {citations} not found.\"}\n",
    "        \n",
    "    context = \"\\n\\n\".join([f\"PARAGRAPH {p['display_id']}:\\n{p['text']}\" for p in cited_paragraphs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    You are a meticulous fact-checker. Determine if the 'ANSWER' is fully supported by the 'SOURCE PARAGRAPHS'.\n",
    "    The answer is 'faithful' only if every single piece of information it contains is directly stated or logically derived from the source paragraphs.\n",
    "    \n",
    "    QUESTION: \"{question}\"\n",
    "    ANSWER TO VERIFY: \"{answer}\"\n",
    "    SOURCE PARAGRAPHS:\n",
    "    {context}\n",
    "    \n",
    "    Respond with a JSON object: {{\"is_faithful\": boolean, \"explanation\": \"brief reasoning\"}}.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = local_llm_generate(prompt, model, tokenizer)\n",
    "\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"eval_faithfulness\", \"model\": EVALUATION_MODEL, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "\n",
    "    return parse_json_from_response(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relevance-score-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_answer_relevance(question: str, answer: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scores the relevance of the generated answer to the original question.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating Answer Relevance ---\")\n",
    "    prompt = f\"\"\"\n",
    "    Score how well the 'ANSWER' addresses the 'ORIGINAL QUESTION' on a scale from 0.0 to 1.0.\n",
    "    - A score of 1.0 means the answer completely and directly answers the question.\n",
    "    - A score of 0.0 means the answer is completely irrelevant.\n",
    "    \n",
    "    ORIGINAL QUESTION: \"{question}\"\n",
    "    ANSWER: \"{answer}\"\n",
    "    \n",
    "    Respond with a JSON object: {{\"score\": float, \"justification\": \"brief reasoning\"}}.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = local_llm_generate(prompt, model, tokenizer)\n",
    "\n",
    "    latency = time.time() - start_time\n",
    "    \n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"eval_answer_relevance\", \"model\": EVALUATION_MODEL, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "    \n",
    "    parsed = parse_json_from_response(response_text)\n",
    "    return {\"score\": parsed.get(\"score\", 0.0), \"justification\": parsed.get(\"justification\", \"\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retrieval-score-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_retrieval_relevance(question: str, paragraphs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scores the relevance of the retrieved documents to the question.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Evaluating Retrieval Relevance ---\")\n",
    "    context = \"\\n\\n\".join([f\"PARAGRAPH {p.get('display_id', p['id'])}:\\n{p['text'][:500]}...\" for p in paragraphs])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Score how relevant the provided 'RETRIEVED PARAGRAPHS' are for answering the 'ORIGINAL QUESTION' on a scale from 0.0 to 1.0.\n",
    "    - A score of 1.0 means the paragraphs contain all the necessary information.\n",
    "    - A score of 0.0 means the paragraphs are completely irrelevant.\n",
    "    \n",
    "    ORIGINAL QUESTION: \"{question}\"\n",
    "    RETRIEVED PARAGRAPHS:\n",
    "    {context}\n",
    "    \n",
    "    Respond with a JSON object: {{\"score\": float, \"justification\": \"brief reasoning\"}}.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = local_llm_generate(prompt, model, tokenizer)\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    response_text = response.choices[0].message.content\n",
    "    p_tokens, c_tokens = response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "    metrics_log.append({\"step\": \"eval_retrieval_relevance\", \"model\": EVALUATION_MODEL, \"latency_s\": latency, \"prompt_tokens\": p_tokens, \"completion_tokens\": c_tokens, \"total_tokens\": p_tokens + c_tokens})\n",
    "    \n",
    "    parsed = parse_json_from_response(response_text)\n",
    "    return {\"score\": parsed.get(\"score\", 0.0), \"justification\": parsed.get(\"justification\", \"\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "run-evaluations",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Evaluating Answer Faithfulness ---\n",
      "\n",
      "--- Evaluating Answer Relevance ---\n",
      "\n",
      "--- Evaluating Retrieval Relevance ---\n",
      "\n",
      "--- QUALITATIVE EVALUATION SUMMARY ---\n",
      "Faithfulness Check: FAILED\n",
      "  -> Explanation: The answer includes information about the Board granting the motion to compel if it determines the responding party has failed to comply and the requesting party has made a good faith effort to resolve the dispute, which is not explicitly stated in the source paragraphs. Additionally, the answer mentions the motion must be in writing and specify the discovery requests, which is not directly supported by the provided source paragraphs.\n",
      "Answer Relevance Score: 0.80\n",
      "  -> Justification: The answer provides detailed information on the requirements for filing a motion to compel discovery, including deadlines, certifications, and the necessity of demonstrating non-compliance. However, it does not explicitly mention formatting details or signature requirements, which are part of the original question.\n",
      "Retrieval Relevance Score: 0.20\n",
      "  -> Justification: The retrieved paragraphs primarily discuss general discovery procedures, sanctions, and protective orders, but they do not specifically address the requirements for filing a motion to compel discovery, including formatting and signatures. The information is tangential and lacks the necessary details to answer the original question.\n"
     ]
    }
   ],
   "source": [
    "# Run all qualitative evaluations\n",
    "faithfulness_result = evaluate_faithfulness(\n",
    "    sample_question, \n",
    "    final_answer_result['answer'], \n",
    "    final_answer_result['citations'],\n",
    "    navigation_result['paragraphs']\n",
    ")\n",
    "\n",
    "answer_relevance_result = evaluate_answer_relevance(\n",
    "    sample_question,\n",
    "    final_answer_result['answer']\n",
    ")\n",
    "\n",
    "retrieval_relevance_result = evaluate_retrieval_relevance(\n",
    "    sample_question,\n",
    "    navigation_result['paragraphs']\n",
    ")\n",
    "\n",
    "print(\"\\n--- QUALITATIVE EVALUATION SUMMARY ---\")\n",
    "print(f\"Faithfulness Check: {'PASSED' if faithfulness_result.get('is_faithful') else 'FAILED'}\")\n",
    "print(f\"  -> Explanation: {faithfulness_result.get('explanation')}\")\n",
    "print(f\"Answer Relevance Score: {answer_relevance_result.get('score'):.2f}\")\n",
    "print(f\"  -> Justification: {answer_relevance_result.get('justification')}\")\n",
    "print(f\"Retrieval Relevance Score: {retrieval_relevance_result.get('score'):.2f}\")\n",
    "print(f\"  -> Justification: {retrieval_relevance_result.get('justification')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-analysis-md",
   "metadata": {},
   "source": [
    "## 7. Final Analysis and Summary\n",
    "\n",
    "Finally, we'll consolidate all our metrics—both operational and qualitative—into two clear summaries. The first DataFrame provides a detailed breakdown of each API call, while the second offers a high-level summary of the entire query's performance and quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "price-config-md-2",
   "metadata": {},
   "source": [
    "### 7.1. Define Model Pricing\n",
    "\n",
    "Define the cost per million tokens for all models used. You should get this information from your LLM provider. The prices below are placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "price-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prices_per_million_tokens = {\n",
    "    \"meta-llama/Meta-Llama-3.1-8B-Instruct\": {\n",
    "        \"input\": 0.02,\n",
    "        \"output\": 0.06\n",
    "    },\n",
    "    \"meta-llama/Llama-3.3-70B-Instruct\": {\n",
    "        \"input\": 0.13,\n",
    "        \"output\": 0.40\n",
    "    },\n",
    "    \"deepseek-ai/DeepSeek-V3\": {\n",
    "        \"input\": 0.50,\n",
    "        \"output\": 1.50\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-dataframe-md",
   "metadata": {},
   "source": [
    "### 7.2. Per-Step Operational Metrics\n",
    "\n",
    "This first DataFrame shows the detailed operational cost and latency for every individual LLM call made during the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dataframe-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Per-Step Performance and Cost Analysis ---\n",
      "                       step                                  model  latency_s  prompt_tokens  completion_tokens  total_tokens  cost_usd\n",
      "0      route_depth_0_reason  meta-llama/Meta-Llama-3.1-8B-Instruct  14.847801           6139                734          6873  0.000167\n",
      "1      route_depth_0_select  meta-llama/Meta-Llama-3.1-8B-Instruct   0.879542           6877                 12          6889  0.000138\n",
      "2      route_depth_1_reason  meta-llama/Meta-Llama-3.1-8B-Instruct   5.627172           3706                330          4036  0.000094\n",
      "3      route_depth_1_select  meta-llama/Meta-Llama-3.1-8B-Instruct   0.698597           3299                 18          3317  0.000067\n",
      "4                 synthesis      meta-llama/Llama-3.3-70B-Instruct   9.360613          12301                265         12566  0.001705\n",
      "5         eval_faithfulness                deepseek-ai/DeepSeek-V3   3.267365           1556                 97          1653  0.000924\n",
      "6     eval_answer_relevance                deepseek-ai/DeepSeek-V3   2.444127            340                 69           409  0.000273\n",
      "7  eval_retrieval_relevance                deepseek-ai/DeepSeek-V3   3.081374           4755                 70          4825  0.002482\n"
     ]
    }
   ],
   "source": [
    "if metrics_log:\n",
    "    df_metrics = pd.DataFrame(metrics_log)\n",
    "\n",
    "    def calculate_cost(row):\n",
    "        model_name = row['model']\n",
    "        prices = model_prices_per_million_tokens.get(model_name, {\"input\": 0, \"output\": 0})\n",
    "        input_cost = (row['prompt_tokens'] / 1_000_000) * prices['input']\n",
    "        output_cost = (row['completion_tokens'] / 1_000_000) * prices['output']\n",
    "        return input_cost + output_cost\n",
    "\n",
    "    df_metrics['cost_usd'] = df_metrics.apply(calculate_cost, axis=1)\n",
    "    \n",
    "    print(\"--- Per-Step Performance and Cost Analysis ---\")\n",
    "    print(df_metrics.to_string())\n",
    "else:\n",
    "    print(\"No metrics were logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-dataframe-md",
   "metadata": {},
   "source": [
    "### 7.3. Final Query Summary\n",
    "\n",
    "This final summary DataFrame provides a holistic, single-row view of the entire query, combining operational totals with the crucial qualitative scores to assess overall success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "summary-dataframe-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Query Summary ---\n",
      "                                                                      Result\n",
      "question                   What are the requirements for filing a motion ...\n",
      "total_latency_s                                                     40.20659\n",
      "total_cost_usd                                                      0.005851\n",
      "total_tokens                                                           40568\n",
      "faithfulness_check                                                    FAILED\n",
      "answer_relevance_score                                                   0.8\n",
      "retrieval_relevance_score                                                0.2\n",
      "overall_confidence_score                                                 0.0\n"
     ]
    }
   ],
   "source": [
    "if metrics_log:\n",
    "    # Calculate totals from the detailed metrics log\n",
    "    total_latency = df_metrics['latency_s'].sum()\n",
    "    total_cost = df_metrics['cost_usd'].sum()\n",
    "    total_tokens = df_metrics['total_tokens'].sum()\n",
    "    \n",
    "    # Get qualitative scores\n",
    "    faithfulness_score = 1.0 if faithfulness_result.get('is_faithful') else 0.0\n",
    "    answer_relevance_score = answer_relevance_result.get('score', 0.0)\n",
    "    retrieval_relevance_score = retrieval_relevance_result.get('score', 0.0)\n",
    "    \n",
    "    # Calculate a simple overall confidence score\n",
    "    overall_confidence = faithfulness_score * answer_relevance_score * retrieval_relevance_score\n",
    "\n",
    "    # Create summary dictionary\n",
    "    summary_data = {\n",
    "        'question': [sample_question],\n",
    "        'total_latency_s': [total_latency],\n",
    "        'total_cost_usd': [total_cost],\n",
    "        'total_tokens': [total_tokens],\n",
    "        'faithfulness_check': ['PASSED' if faithfulness_score == 1.0 else 'FAILED'],\n",
    "        'answer_relevance_score': [answer_relevance_score],\n",
    "        'retrieval_relevance_score': [retrieval_relevance_score],\n",
    "        'overall_confidence_score': [overall_confidence]\n",
    "    }\n",
    "    \n",
    "    df_summary = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(\"--- Final Query Summary ---\")\n",
    "    # Transpose for better readability of a single-row summary\n",
    "    print(df_summary.T.rename(columns={0: 'Result'}))\n",
    "else:\n",
    "    print(\"Cannot generate summary as no metrics were logged.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion-cell",
   "metadata": {},
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "This notebook has demonstrated an end-to-end agentic RAG workflow using customizable, open-source LLMs. By employing a hierarchical navigation strategy and a multi-step qualitative evaluation, we can tackle complex questions in long documents with high precision and full traceability, all without the overhead of a traditional vector database.\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1.  **Agents Offer Control:** Breaking the problem into specialized agents (Router, Synthesizer, Evaluator) provides greater control and allows for the use of different models optimized for each task.\n",
    "2.  **Hierarchical Navigation is Powerful:** This approach effectively narrows down a vast search space, mimicking human-like research patterns.\n",
    "3.  **LLM-based Evaluation is Crucial:** Moving beyond simple operational metrics to assess faithfulness and relevance is key to building trustworthy and reliable AI systems.\n",
    "4.  **Comprehensive Analysis:** Consolidating operational and qualitative metrics into a final summary provides a clear, holistic view of system performance for each query."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
